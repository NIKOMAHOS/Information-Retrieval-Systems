{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"..\\\\requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: !\n",
    "3210122 + 3210191 = 6420313\n",
    "- So we get the `trec_covid` IR2025 collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass))\n",
    "\n",
    "if es.ping():\n",
    "    print(\"‚úÖ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"‚ùå Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Index 'ir2025-index' already exists.\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    print(f\"‚úÖ Index '{INDEX_NAME}' already exists.\")\n",
    "\n",
    "else:\n",
    "    # Define the settings and mappings for the index\n",
    "    settings = {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                },\n",
    "                \"english_stemmer\": {\n",
    "                    \"type\": \"kstem\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_english\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\", # Converts all terms to lowercase\n",
    "                        \"english_stop\", # Removes English stop words\n",
    "                        \"english_stemmer\" # Reduces words to their root form usign kstem\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    mappings = {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_english\",\n",
    "                \"similarity\": \"BM25\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the index with the specified settings and mappings\n",
    "    es.indices.create(\n",
    "        index=INDEX_NAME,\n",
    "        settings=settings,\n",
    "        mappings=mappings\n",
    "    )\n",
    "    print(f\"‚úÖ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/trec-covid/'\n",
    "\n",
    "with jsonlines.open(input_dir + 'corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate custom_english Analyzer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # KrovetzStemmer supports up to python 3.10 at best \n",
    "import string\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer() # It's \"Closer\" to Korvetz than Snowball is\n",
    "\n",
    "def es_like_preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower().strip()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove invisible or non-ASCII chars (TODO: SEEE IF TOU CAN ADD IT TO ElasticSearch AND THE OTHER PHASES)\n",
    "    #text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords, apply stemming (Porter), (TODO: SEEE IF TOU CAN ADD IT TO ElasticSearch AND THE OTHER PHASES) skip anything not purely arithmetic\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words or not token.isalpha()]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus into sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171332/171332 [08:47<00:00, 325.01doc/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# Final list of tokenized sentences\n",
    "processed_sentences = []\n",
    "print(\"Preprocessing corpus into sentences...\")\n",
    "for doc in tqdm(corpus, unit=\"doc\"): # ~8 minutes\n",
    "    doc_text = doc[\"text\"]\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    for sentence in sentences:\n",
    "        tokens = es_like_preprocess(sentence).split() \n",
    "        if tokens:  # skip empty\n",
    "            processed_sentences.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# --- Train Word2Vec --- # ~15 minutes\n",
    "model = Word2Vec(processed_sentences, vector_size=200, window=5, min_count=5, epochs=15, negative=10, sample=1e-4, sg=1, workers=6, seed=42) # Use skip-gram = 1 | Use CBOW 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 words with smallest vector norm:\n",
      "wwwactabiomedicait: 0.0384\n",
      "bioeng: 0.0396\n",
      "protoc: 1.2187\n",
      "briq: 1.4147\n",
      "amer: 1.6496\n",
      "physiol: 1.7822\n",
      "bjog: 1.8216\n",
      "subchapt: 1.8225\n",
      "patientssubject: 1.8352\n",
      "highinfect: 1.8380\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vec_norms = np.linalg.norm(model.wv.vectors, axis=1)\n",
    "sorted_words = sorted(zip(model.wv.index_to_key, vec_norms), key=lambda x: x[1])\n",
    "print(\"10 words with smallest vector norm:\")\n",
    "for word, norm in sorted_words[:10]:\n",
    "    print(f\"{word}: {norm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Word2Vec Hyperparameter Summary\n",
    "\n",
    "| Parameter     | Chosen Value | Purpose                                   | Pros                                              | Cons                                               |\n",
    "|---------------|-------------------|-------------------------------------------|---------------------------------------------------|----------------------------------------------------|\n",
    "| `vector_size` | 200               | Dimensionality of word embeddings         | Captures semantic nuances                         | Higher means more computational cost                          |\n",
    "| `window`      | 5                 | Context window size                       | Balances syntactic and semantic information       | Too large may introduce noise                      |\n",
    "| `min_count`   | 5                 | Minimum frequency threshold               | Removes rare noise words                          | May exclude rare but important terms               |\n",
    "| `sg`          | 1 (Skip-Gram)     | Training algorithm                        | Better for rare words                             | Slower training                                    |\n",
    "| `epochs`      | 15                | Number of training iterations             | Improves convergence                              | Risk of overfitting with too many epochs           |\n",
    "| `negative`    | 10                | Number of negative samples                | Enhances embedding quality                        | Too many can slow training                         |\n",
    "| `sample`      | 1e-4              | Subsampling frequent words                | Reduces dominance of frequent/common words        | May remove useful frequent words if too aggressive |\n",
    "| `workers`     | 6         | Number of parallel training threads       | Speeds up training                                | Overhead with too many threads                     |\n",
    "| `seed`        | 42                | Random seed for reproducibility           | Ensures consistent results                        | None                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# --- Save model ---\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save only the KeyedVectors part\n",
    "model.wv.save(\"../models/w2v_ir2025.kv\")\n",
    "print(\"‚úÖ Word2Vec model saved (only KeyedVectors) to: ../models/w2v_ir2025.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Word2Vec model (only KeyedVectors) successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# --- Load model ---\n",
    "kv_model = KeyedVectors.load(\"../models/w2v_ir2025.kv\", mmap='r')\n",
    "print(\"‚úÖ Word2Vec model (only KeyedVectors) successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65459\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(kv_model.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean vector norm: 3.7523\n",
      "Max vector norm: 7.7379\n",
      "Min vector norm: 0.0384\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_norms = np.linalg.norm(kv_model.vectors, axis=1)\n",
    "print(f\"Mean vector norm: {np.mean(vector_norms):.4f}\")\n",
    "print(f\"Max vector norm: {np.max(vector_norms):.4f}\")\n",
    "print(f\"Min vector norm: {np.min(vector_norms):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_word2vec(query_text, kv_model, topn=3):\n",
    "    \"\"\"\n",
    "    Expands a query by finding similar terms using Word2Vec model.\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): Original query text\n",
    "        kv_model: Loaded KeyedVectors model\n",
    "        topn (int): Number of similar terms to add per query term\n",
    "        \n",
    "    Returns:\n",
    "        str: Expanded query text\n",
    "    \"\"\"\n",
    "    # Preprocess query same way as corpus\n",
    "    query_tokens = es_like_preprocess(query_text).split()\n",
    "    \n",
    "    expanded_terms = []\n",
    "\n",
    "    for token in query_tokens:\n",
    "        expanded_terms.append(token)\n",
    "\n",
    "        if token in kv_model.key_to_index:\n",
    "            similar_terms = kv_model.most_similar(token, topn=topn)\n",
    "            for term, score in similar_terms:\n",
    "                if score >= 0.7 and term.isalpha():  # add only if clean and relevant\n",
    "                    expanded_terms.append(term)\n",
    "\n",
    "    return \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 queries.\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(input_dir + 'queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "    print(f\"Loaded {len(queries)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?query/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 34.41query/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "print(\"Expanding Queries..\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_word2vec(query[\"text\"], kv_model)\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Expanded queries saved to ../data/trec-covid/queries_expanded_word2vec.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_word2vec.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write(q)\n",
    "    print(\"‚úÖ Expanded queries saved to ../data/trec-covid/queries_expanded_word2vec.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 20:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 20: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:02<00:00, 21.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to: ../results/phase_3/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 30: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 46.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to: ../results/phase_3/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:01<00:00, 32.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to: ../results/phase_3/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_3(expanded_queries_path):\n",
    "    # Load queries\n",
    "    with open(expanded_queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_3\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Expanded Queries with Word2Vec for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"expanded_text\"] # already did this: expand_query_with_word2vec()\n",
    "            \n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            { \"match\": { \"text\": query_text }}\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                size=k\n",
    "            )\n",
    "\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"‚úÖ Results saved to: ../results/phase_3/retrieval_top_{k}.json\")\n",
    "\n",
    "    return runs\n",
    "    \n",
    "runs = process_queries_phase_3(\"../data/trec-covid/queries_expanded_word2vec.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "‚úÖ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_20.json\n",
      "‚úÖ Average metrics saved to: ../results\\phase_3\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "‚úÖ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_30.json\n",
      "‚úÖ Average metrics saved to: ../results\\phase_3\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "‚úÖ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_50.json\n",
      "‚úÖ Average metrics saved to: ../results\\phase_3\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "            # num_docs = len(docs)\n",
    "            # print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"‚úÖ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"‚úÖ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "        \n",
    "compute_metrics(qrels, runs, \"phase_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare (e.g. [20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (e.g. ['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"‚ö†Ô∏è File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 MAP</th>\n",
       "      <th>Phase 2 avgPre@5</th>\n",
       "      <th>Phase 2 avgPre@10</th>\n",
       "      <th>Phase 2 avgPre@15</th>\n",
       "      <th>Phase 2 avgPre@20</th>\n",
       "      <th>Phase 3 MAP</th>\n",
       "      <th>Phase 3 avgPre@5</th>\n",
       "      <th>Phase 3 avgPre@10</th>\n",
       "      <th>Phase 3 avgPre@15</th>\n",
       "      <th>Phase 3 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.028280</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.008953</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.013308</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.020569              0.64              0.582              0.564   \n",
       "30     0.027753              0.64              0.582              0.564   \n",
       "50     0.039911              0.64              0.582              0.564   \n",
       "\n",
       "    Phase 1 avgPre@20  Phase 2 MAP  Phase 2 avgPre@5  Phase 2 avgPre@10  \\\n",
       "k                                                                         \n",
       "20              0.548     0.020473             0.604              0.586   \n",
       "30              0.549     0.028280             0.604              0.586   \n",
       "50              0.549     0.040742             0.604              0.586   \n",
       "\n",
       "    Phase 2 avgPre@15  Phase 2 avgPre@20  Phase 3 MAP  Phase 3 avgPre@5  \\\n",
       "k                                                                         \n",
       "20           0.554667              0.536     0.006545             0.252   \n",
       "30           0.554667              0.536     0.008953             0.252   \n",
       "50           0.554667              0.536     0.013308             0.252   \n",
       "\n",
       "    Phase 3 avgPre@10  Phase 3 avgPre@15  Phase 3 avgPre@20  \n",
       "k                                                            \n",
       "20              0.224              0.224              0.229  \n",
       "30              0.224              0.224              0.229  \n",
       "50              0.224              0.224              0.229  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    \"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    # \"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}\n",
    "_ = compare_phases(phases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
