{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Phase 3: Query Expansion with Word2Vec Synonyms on the IR2025 Collection\n",
    "\n",
    "\n",
    "TODO\n",
    "---\n",
    "> Maria Schoinaki, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210191@aueb.gr <br/><br/>\n",
    "\n",
    "> Nikos Mitsakis, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210122@aueb.gr <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r \"..\\\\requirements.txt\" \n",
    "# fix path accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass), request_timeout=30, retry_on_timeout=True, max_retries=10)\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'ir2025-index' deleted\n",
      "✅ Index 'ir2025-index' created\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"✅ Index '{INDEX_NAME}' deleted\")\n",
    "\n",
    "# Define the settings and mappings for the index\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"english_stop\": {\n",
    "                \"type\": \"stop\",\n",
    "                \"stopwords\": \"_english_\"\n",
    "            },\n",
    "            \"english_stemmer\": {\n",
    "                \"type\": \"kstem\"\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"custom_english\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\", # Converts all terms to lowercase\n",
    "                    \"english_stop\", # Removes English stop words\n",
    "                    \"english_stemmer\" # Reduces words to their root form usign kstem\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"doc_id\": {\"type\": \"keyword\"},\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_english\",\n",
    "            \"similarity\": \"BM25\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index with the specified settings and mappings\n",
    "es.indices.create(\n",
    "    index=INDEX_NAME,\n",
    "    settings=settings,\n",
    "    mappings=mappings\n",
    ")\n",
    "print(f\"✅ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion  \n",
    "Using the `streaming_bulk` helper, we ingest all IR2025 documents in chunks of 500.  \n",
    "A progress bar (tqdm) provides real‐time feedback on indexing throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171332/171332 [00:44<00:00, 3877.28docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 171332/171332 documents into 'ir2025-index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import streaming_bulk\n",
    "\n",
    "# Generator function to yield documents\n",
    "def generate_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            yield {\n",
    "                \"_index\": INDEX_NAME,\n",
    "                \"_id\": doc[\"_id\"],\n",
    "                \"_source\": {\n",
    "                    \"doc_id\": doc[\"_id\"],\n",
    "                    \"text\": doc[\"text\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Count the total number of documents for the progress bar\n",
    "with open(\"../data/trec-covid/corpus.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    total_docs = sum(1 for _ in f)\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress = tqdm(unit=\"docs\", total=total_docs)\n",
    "\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=es, actions=generate_documents(\"../data/trec-covid/corpus.jsonl\"), chunk_size=500):\n",
    "    progress.update(1)\n",
    "    successes += int(ok)\n",
    "\n",
    "progress.close()\n",
    "print(f\"✅ Indexed {successes}/{total_docs} documents into '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLTK Setup and Corpus Preprocessing\n",
    "\n",
    "Download required NLTK data, load the IR2025 corpus into memory, and define a Python function to simulate the `custom_english` analyzer for downstream TF–IDF modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the necessary NLTK corpora and models.\n",
    "\n",
    "- **Tokenization & POS Tagging**: `punkt_tab`, `averaged_perceptron_tagger`  \n",
    "- **Stopword List**: `stopwords`  \n",
    "- **Lexical Database**: `wordnet` and the multilingual WordNet (`omw-1.4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IR2025 corpus into memory as a list of JSON objects using `jsonlines.open`, preparing it for TF–IDF vectorization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../data/trec-covid/corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate `custom_english` Analyzer in Python\n",
    "\n",
    "Here we replicate our Elasticsearch `custom_english` analyzer pipeline in pure Python using NLTK:\n",
    "\n",
    "1. **Lowercase & Trim**  \n",
    "2. **Punctuation Removal**  \n",
    "3. **Tokenization** (`word_tokenize`)  \n",
    "4. **Stopword Filtering** (`stopwords.words('english')`)  \n",
    "5. **Stemming** (PorterStemmer as a proxy for Krovetz)  \n",
    "\n",
    "This function lets us preprocess text identically before building the TF–IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate custom_english Analyzer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # KrovetzStemmer supports up to python 3.10 at best \n",
    "import string\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer() # It's \"Closer\" to Korvetz than Snowball is\n",
    "\n",
    "def es_like_preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower().strip()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply stemming (Porter)\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words or not token.isalpha()]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF–IDF Model Construction and Persistence\n",
    "\n",
    "In this section, we build a TF–IDF model over the preprocessed IR2025 corpus, compute key statistics, and save all artifacts for later use:\n",
    "\n",
    "- **Preprocessing Statistics**: total tokens, unique tokens, average tokens per document  \n",
    "- **TF–IDF Statistics**: vocabulary size, average/min/max IDF  \n",
    "- **Model Artifacts**:  \n",
    "  - `tfidf_vectorizer.joblib` (fitted `TfidfVectorizer`)  \n",
    "  - `idf_scores.json` (term → IDF)  \n",
    "  - `tfidf_statistics.json` (all collected statistics)\n",
    "\n",
    "We implement three functions:\n",
    "\n",
    "1. `build_and_save_tfidf_model(corpus, output_dir)`  \n",
    "   - Preprocesses each document, updates statistics  \n",
    "   - Fits `TfidfVectorizer`, extracts IDF scores  \n",
    "   - Saves model and statistics to disk  \n",
    "\n",
    "2. `load_tfidf_model(output_dir)`  \n",
    "   - Loads the saved vectorizer, IDF scores, and statistics  \n",
    "   - Prints summary for validation  \n",
    "\n",
    "3. `transform_text(text, vectorizer)`  \n",
    "   - Applies the loaded vectorizer to new text, returning its TF–IDF representation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# import joblib\n",
    "# import json\n",
    "# import os\n",
    "\n",
    "# def build_and_save_tfidf_model(corpus, output_dir=\"../models\"):\n",
    "#     \"\"\"\n",
    "#     Build TF-IDF model from corpus, compute statistics, and save the model.\n",
    "    \n",
    "#     Args:\n",
    "#         corpus: List of documents with 'text' field\n",
    "#         output_dir: Directory to save models and statistics\n",
    "    \n",
    "#     Returns:\n",
    "#         tuple: (vectorizer, idf_scores, statistics_dict)\n",
    "#     \"\"\"\n",
    "#     # Create output directory\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Initialize counters for statistics\n",
    "#     total_tokens = 0\n",
    "#     unique_tokens = set()\n",
    "#     statistics = {}\n",
    "\n",
    "#     # Preprocess with detailed statistics\n",
    "#     print(\"Preprocessing corpus...\")\n",
    "#     preprocessed_corpus = []\n",
    "#     for doc in tqdm(corpus, desc=\"Preprocessing documents\", unit=\"doc\"):\n",
    "#         processed_text = es_like_preprocess(doc[\"text\"])\n",
    "#         tokens = processed_text.split()\n",
    "        \n",
    "#         # Update statistics\n",
    "#         total_tokens += len(tokens)\n",
    "#         unique_tokens.update(tokens)\n",
    "        \n",
    "#         preprocessed_corpus.append(processed_text)\n",
    "\n",
    "#     # Save preprocessing statistics\n",
    "#     statistics['preprocessing'] = {\n",
    "#         'total_tokens': total_tokens,\n",
    "#         'unique_tokens': len(unique_tokens),\n",
    "#         'average_tokens_per_doc': total_tokens/len(corpus)\n",
    "#     }\n",
    "\n",
    "#     print(f\"\\nPreprocessing statistics:\")\n",
    "#     print(f\"- Total tokens: {total_tokens:,}\")\n",
    "#     print(f\"- Unique tokens: {len(unique_tokens):,}\")\n",
    "#     print(f\"- Average tokens per document: {total_tokens/len(corpus):,.1f}\")\n",
    "\n",
    "#     # Build TF-IDF model with detailed progress\n",
    "#     print(\"\\nBuilding TF-IDF model...\")\n",
    "#     tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "#     with tqdm(total=3, desc=\"TF-IDF computation\") as pbar:\n",
    "#         # Fit the vectorizer\n",
    "#         tfidf_vectorizer.fit(preprocessed_corpus)\n",
    "#         pbar.update(1)\n",
    "        \n",
    "#         # Get feature names\n",
    "#         feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "#         pbar.update(1)\n",
    "        \n",
    "#         # Calculate IDF scores\n",
    "#         idf_scores = dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "#         pbar.update(1)\n",
    "\n",
    "#     # Calculate and save TF-IDF statistics\n",
    "#     idf_values = list(idf_scores.values())\n",
    "#     statistics['tfidf'] = {\n",
    "#         'vocabulary_size': len(idf_scores),\n",
    "#         'average_idf': float(np.mean(idf_values)),\n",
    "#         'max_idf': float(max(idf_values)),\n",
    "#         'min_idf': float(min(idf_values))\n",
    "#     }\n",
    "\n",
    "#     print(\"\\nTF-IDF statistics:\")\n",
    "#     print(f\"- Vocabulary size: {len(idf_scores):,} terms\")\n",
    "#     print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "#     print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "#     print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    "\n",
    "#     # Save everything\n",
    "#     print(\"\\nSaving models and statistics...\")\n",
    "#     try:\n",
    "#         # Save vectorizer\n",
    "#         joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "#         # Save IDF scores\n",
    "#         with open(os.path.join(output_dir, 'idf_scores.json'), 'w', encoding='utf-8') as f:\n",
    "#             json.dump(idf_scores, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "#         # Save statistics\n",
    "#         with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'w', encoding='utf-8') as f:\n",
    "#             json.dump(statistics, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "#         print(\"\\n✅ Saved successfully:\")\n",
    "#         print(f\"- Vectorizer: {os.path.join(output_dir, 'tfidf_vectorizer.joblib')}\")\n",
    "#         print(f\"- IDF scores: {os.path.join(output_dir, 'idf_scores.json')}\")\n",
    "#         print(f\"- Statistics: {os.path.join(output_dir, 'tfidf_statistics.json')}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n❌ Error saving files: {e}\")\n",
    "        \n",
    "#     return tfidf_vectorizer, idf_scores\n",
    "\n",
    "# # Function to load the saved model\n",
    "# def load_tfidf_model(output_dir=\"../models\"):\n",
    "#     \"\"\"\n",
    "#     Load the saved TF-IDF model, IDF scores, and statistics.\n",
    "    \n",
    "#     Args:\n",
    "#         output_dir: Directory where models and statistics are saved\n",
    "        \n",
    "#     Returns:\n",
    "#         tuple: (vectorizer, idf_scores, statistics)\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         # Load vectorizer\n",
    "#         vectorizer = joblib.load(os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "#         # Load IDF scores\n",
    "#         with open(os.path.join(output_dir, 'idf_scores.json'), 'r', encoding='utf-8') as f:\n",
    "#             idf_scores = json.load(f)\n",
    "            \n",
    "#         # Load statistics\n",
    "#         with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'r', encoding='utf-8') as f:\n",
    "#             statistics = json.load(f)\n",
    "        \n",
    "#         print(\"\\nModel validation:\")\n",
    "#         print(f\"- Vocabulary size: {len(idf_scores):,}\")\n",
    "#         print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "#         print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "#         print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    " \n",
    "#         print(\"✅ Model loaded successfully\")\n",
    "#         return vectorizer, idf_scores\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading model: {e}\")\n",
    "#         return None, None\n",
    "        \n",
    "# # Transform new text using the loaded vectorizer\n",
    "# def transform_text(text, vectorizer):\n",
    "#     \"\"\"Transform new text using the loaded vectorizer\"\"\"\n",
    "#     try:\n",
    "#         transformed = vectorizer.transform([text])\n",
    "#         return transformed\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error transforming text: {e}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_and_save_tfidf_model(corpus, output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Build TF-IDF model from corpus, compute statistics, and save the model.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of documents with 'text' field\n",
    "        output_dir: Directory to save models and statistics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics_dict)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize counters for statistics\n",
    "    all_tokens = []\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "    statistics = {}\n",
    "\n",
    "    # Preprocess with detailed statistics\n",
    "    print(\"Preprocessing corpus...\")\n",
    "    preprocessed_corpus = []\n",
    "    for doc in tqdm(corpus, desc=\"Preprocessing documents\", unit=\"doc\"):\n",
    "        processed_text = es_like_preprocess(doc[\"text\"])\n",
    "        tokens = processed_text.split()\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "        # Update statistics\n",
    "        total_tokens += len(tokens)\n",
    "        unique_tokens.update(tokens)\n",
    "        \n",
    "        preprocessed_corpus.append(processed_text)\n",
    "    \n",
    "    \n",
    "    word_counts = Counter(all_tokens)\n",
    "    global top_50_words\n",
    "    top_50_words = [w for w, _ in word_counts.most_common(50)]\n",
    "    print(\"Top 50 words:\", top_50_words)\n",
    "\n",
    "    # Save preprocessing statistics\n",
    "    statistics['preprocessing'] = {\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': len(unique_tokens),\n",
    "        'average_tokens_per_doc': total_tokens/len(corpus)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nPreprocessing statistics:\")\n",
    "    print(f\"- Total tokens: {total_tokens:,}\")\n",
    "    print(f\"- Unique tokens: {len(unique_tokens):,}\")\n",
    "    print(f\"- Average tokens per document: {total_tokens/len(corpus):,.1f}\")\n",
    "\n",
    "    # Build TF-IDF model with detailed progress\n",
    "    print(\"\\nBuilding TF-IDF model...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "    with tqdm(total=3, desc=\"TF-IDF computation\") as pbar:\n",
    "        # Fit the vectorizer\n",
    "        tfidf_vectorizer.fit(preprocessed_corpus)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        idf_scores = dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Calculate and save TF-IDF statistics\n",
    "    idf_values = list(idf_scores.values())\n",
    "    statistics['tfidf'] = {\n",
    "        'vocabulary_size': len(idf_scores),\n",
    "        'average_idf': float(np.mean(idf_values)),\n",
    "        'max_idf': float(max(idf_values)),\n",
    "        'min_idf': float(min(idf_values))\n",
    "    }\n",
    "\n",
    "    print(\"\\nTF-IDF statistics:\")\n",
    "    print(f\"- Vocabulary size: {len(idf_scores):,} terms\")\n",
    "    print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "    print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "    print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    "\n",
    "    # Save everything\n",
    "    print(\"\\nSaving models and statistics...\")\n",
    "    try:\n",
    "        # Save vectorizer\n",
    "        joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Save IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(idf_scores, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Save statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(statistics, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(\"\\n✅ Saved successfully:\")\n",
    "        print(f\"- Vectorizer: {os.path.join(output_dir, 'tfidf_vectorizer.joblib')}\")\n",
    "        print(f\"- IDF scores: {os.path.join(output_dir, 'idf_scores.json')}\")\n",
    "        print(f\"- Statistics: {os.path.join(output_dir, 'tfidf_statistics.json')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error saving files: {e}\")\n",
    "        \n",
    "    return tfidf_vectorizer, idf_scores\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_tfidf_model(output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Load the saved TF-IDF model, IDF scores, and statistics.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory where models and statistics are saved\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorizer\n",
    "        vectorizer = joblib.load(os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Load IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'r', encoding='utf-8') as f:\n",
    "            idf_scores = json.load(f)\n",
    "            \n",
    "        # Load statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'r', encoding='utf-8') as f:\n",
    "            statistics = json.load(f)\n",
    "        \n",
    "        print(\"\\nModel validation:\")\n",
    "        print(f\"- Vocabulary size: {len(idf_scores):,}\")\n",
    "        print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "        print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "        print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    " \n",
    "        print(\"✅ Model loaded successfully\")\n",
    "        return vectorizer, idf_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "# Transform new text using the loaded vectorizer\n",
    "def transform_text(text, vectorizer):\n",
    "    \"\"\"Transform new text using the loaded vectorizer\"\"\"\n",
    "    try:\n",
    "        transformed = vectorizer.transform([text])\n",
    "        return transformed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load or Build TF–IDF Model  \n",
    "\n",
    "Attempt to load the saved TF–IDF vectorizer and IDF scores. If they are not found, build the model from the corpus and save the artifacts for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing documents: 100%|██████████| 171332/171332 [06:29<00:00, 439.48doc/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words: ['patient', 'use', 'infect', 'covid19', 'studi', 'diseas', 'result', 'viru', 'cell', 'case', 'sever', 'clinic', 'method', 'health', 'respiratori', 'effect', 'group', 'includ', 'data', 'differ', 'develop', 'treatment', 'protein', 'viral', 'may', 'coronaviru', 'increas', 'model', 'associ', 'test', 'report', 'risk', 'time', 'compar', 'pandem', 'conclus', 'also', 'human', 'control', 'respons', 'system', 'activ', 'p', 'detect', 'show', 'rate', 'sarscov2', 'provid', 'present', 'identifi']\n",
      "\n",
      "Preprocessing statistics:\n",
      "- Total tokens: 16,535,640\n",
      "- Unique tokens: 328,788\n",
      "- Average tokens per document: 96.5\n",
      "\n",
      "Building TF-IDF model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TF-IDF computation: 100%|██████████| 3/3 [00:14<00:00,  4.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF statistics:\n",
      "- Vocabulary size: 298,422 terms\n",
      "- Average IDF: 11.84\n",
      "- Max IDF: 12.36\n",
      "- Min IDF: 2.03\n",
      "\n",
      "Saving models and statistics...\n",
      "\n",
      "✅ Saved successfully:\n",
      "- Vectorizer: ../models\\tfidf_vectorizer.joblib\n",
      "- IDF scores: ../models\\idf_scores.json\n",
      "- Statistics: ../models\\tfidf_statistics.json\n"
     ]
    }
   ],
   "source": [
    "vectorizer, idf_scores = build_and_save_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model validation:\n",
      "- Vocabulary size: 298,422\n",
      "- Average IDF: 11.84\n",
      "- Max IDF: 12.36\n",
      "- Min IDF: 2.03\n",
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Test loading\n",
    "vectorizer, idf_scores = load_tfidf_model()\n",
    "if not vectorizer and not idf_scores:\n",
    "    # Build and save the model\n",
    "    vectorizer, idf_scores = build_and_save_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1126604 tokenized sentences from pickle.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "processed_sentences_path = \"../data/processed_sentences.pkl\"\n",
    "\n",
    "# Try to load processed_sentences from pickle\n",
    "if os.path.exists(processed_sentences_path):\n",
    "    with open(processed_sentences_path, \"rb\") as f:\n",
    "        processed_sentences = pickle.load(f)\n",
    "    print(f\"✅ Loaded {len(processed_sentences)} tokenized sentences from pickle.\")\n",
    "else:\n",
    "    # Process the corpus\n",
    "    processed_sentences = []\n",
    "    print(\"⏳ Preprocessing corpus into sentences...\")\n",
    "    for doc in tqdm(corpus, unit=\"doc\"):\n",
    "        doc_text = doc[\"text\"]\n",
    "        sentences = sent_tokenize(doc_text)\n",
    "        for sentence in sentences:\n",
    "            tokens = es_like_preprocess(sentence).split()\n",
    "            if tokens:\n",
    "                processed_sentences.append(tokens)\n",
    "\n",
    "    # Save to pickle\n",
    "    os.makedirs(os.path.dirname(processed_sentences_path), exist_ok=True)\n",
    "    with open(processed_sentences_path, \"wb\") as f:\n",
    "        pickle.dump(processed_sentences, f)\n",
    "    print(f\"✅ Saved {len(processed_sentences)} tokenized sentences to pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Word2Vec Hyperparameter Summary\n",
    "\n",
    "| Parameter     | Chosen Value | Purpose                                   | Pros                                              | Cons                                               |\n",
    "|---------------|-------------------|-------------------------------------------|---------------------------------------------------|----------------------------------------------------|\n",
    "| `vector_size` | 200               | Dimensionality of word embeddings         | Captures semantic nuances                         | Higher means more computational cost                          |\n",
    "| `window`      | 5                 | Context window size                       | Balances syntactic and semantic information       | Too large may introduce noise                      |\n",
    "| `min_count`   | 5                 | Minimum frequency threshold               | Removes rare noise words                          | May exclude rare but important terms               |\n",
    "| `sg`          | 1 (Skip-Gram)     | Training algorithm                        | Better for rare words                             | Slower training                                    |\n",
    "| `epochs`      | 15                | Number of training iterations             | Improves convergence                              | Risk of overfitting with too many epochs           |\n",
    "| `negative`    | 10                | Number of negative samples                | Enhances embedding quality                        | Too many can slow training                         |\n",
    "| `sample`      | 1e-4              | Subsampling frequent words                | Reduces dominance of frequent/common words        | May remove useful frequent words if too aggressive |\n",
    "| `workers`     | 6         | Number of parallel training threads       | Speeds up training                                | Overhead with too many threads                     |\n",
    "| `seed`        | 42                | Random seed for reproducibility           | Ensures consistent results                        | None                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def is_expandable(pos):\n",
    "    return pos.startswith('NN') or pos.startswith('JJ')  # nouns or adjectives\n",
    "\n",
    "def expand_query_with_word2vec(query_text, kv_model, topn=1, n_expand=1, similarity_threshold=0.6):\n",
    "    \"\"\"\n",
    "    Expand a query using top TF-IDF tokens (nouns/adjectives) and their most similar terms from Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): Original query\n",
    "        kv_model: Trained Word2Vec model (KeyedVectors)\n",
    "        tfidf_vectorizer: Trained TF-IDF vectorizer with idf_\n",
    "        topn (int): Number of similar terms per word to consider\n",
    "        n_expand (int): Number of query terms to expand (highest IDF)\n",
    "        similarity_threshold (float): Minimum similarity score to include term\n",
    "\n",
    "    Returns:\n",
    "        str: Expanded query\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(query_text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    original_words = set(tokens)\n",
    "\n",
    "    # Select candidate tokens (noun/adjective, alphabetic, non-stopword)\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "    \n",
    "    # Get IDF scores from the vectorizer\n",
    "    idf_scores = dict(zip(vectorizer.get_feature_names_out(), vectorizer.idf_))\n",
    "\n",
    "    # Score candidate terms by IDF (after preprocessing for vectorizer lookup)\n",
    "    scored = [\n",
    "        (word, idf_scores.get(es_like_preprocess(word), 0.0))\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    # Pick top-n_expand tokens to expand\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "\n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        if word in kv_model.key_to_index:\n",
    "            try:\n",
    "                similar_terms = kv_model.most_similar(word, topn=topn)\n",
    "                for sim_word, score in similar_terms:\n",
    "                    if (\n",
    "                        #score >= similarity_threshold and\n",
    "                        sim_word.isalpha() and\n",
    "                        sim_word not in original_words\n",
    "                    ):\n",
    "                        expanded_terms.append(sim_word)\n",
    "                        if len(expanded_terms) >= topn:\n",
    "                            break  # only keep at most topn expansions total\n",
    "            except KeyError:\n",
    "                continue  # word not in vocab\n",
    "\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_query_expansion(w2v_model, validation_queries, k_values=[20, 30, 50], topn=3, n_expand=1, similarity_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Expands validation queries using Word2Vec, performs retrieval at multiple cutoff values (k),\n",
    "    and returns the average MAP across all k_values.\n",
    "    \"\"\"\n",
    "    expanded_queries = []\n",
    "    for query in validation_queries:\n",
    "        expanded_text = expand_query_with_word2vec(\n",
    "            query[\"text\"],\n",
    "            w2v_model.wv,\n",
    "            topn=topn,\n",
    "            n_expand=n_expand,\n",
    "            similarity_threshold=similarity_threshold\n",
    "        )\n",
    "        expanded_queries.append({\n",
    "            \"_id\": query[\"_id\"],\n",
    "            \"expanded_text\": expanded_text\n",
    "        })\n",
    "\n",
    "    all_run_results = {}  # {k: {qid: {docid: score}}}\n",
    "    for k in k_values:\n",
    "        run = {}\n",
    "        for query in expanded_queries:\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": { \"text\": query[\"expanded_text\"] }},\n",
    "                size=k\n",
    "            )\n",
    "            run[query[\"_id\"]] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "        all_run_results[k] = run\n",
    "\n",
    "    total_map = 0.0\n",
    "    valid_ks = 0\n",
    "\n",
    "    for k, run in all_run_results.items():\n",
    "        # Filter qrels\n",
    "        qrels_subset = {qid: qrels[qid] for qid in run if qid in qrels}\n",
    "        if not qrels_subset:\n",
    "            continue\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrels_subset, {\"map\"})\n",
    "        results = evaluator.evaluate(run)\n",
    "\n",
    "        if results:\n",
    "            avg_map_k = sum(res[\"map\"] for res in results.values()) / len(results)\n",
    "            total_map += avg_map_k\n",
    "            valid_ks += 1\n",
    "\n",
    "    return total_map / valid_ks if valid_ks > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def average_similarity_top_words(model, top_words):\n",
    "    \"\"\"\n",
    "    Calculate the average pairwise similarity for a list of words\n",
    "    using the Word2Vec model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained gensim Word2Vec model\n",
    "        top_words: List of words to measure similarity between\n",
    "    \n",
    "    Returns:\n",
    "        float: average pairwise similarity\n",
    "    \"\"\"\n",
    "    valid_words = [w for w in top_words if w in model.wv.key_to_index]\n",
    "    pairs = list(combinations(valid_words, 2))\n",
    "    if not pairs:\n",
    "        return 0.0\n",
    "\n",
    "    sims = [model.wv.similarity(w1, w2) for w1, w2 in pairs]\n",
    "    avg_sim = sum(sims) / len(sims)\n",
    "    return avg_sim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 queries.\n",
      "Sampled 20 validation queries.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "with jsonlines.open('../data/trec-covid/queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "\n",
    "# Sample 10 unique random indices (without replacement)\n",
    "idxs = random.sample(range(len(queries)), 20)\n",
    "\n",
    "# Get the corresponding query objects\n",
    "validation_queries = [queries[i] for i in idxs]\n",
    "\n",
    "print(f\"Loaded {len(queries)} queries.\")\n",
    "print(f\"Sampled {len(validation_queries)} validation queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning Word2Vec:   0%|          | 0/30 [00:00<?, ?config/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning Word2Vec: 100%|██████████| 30/30 [3:39:18<00:00, 438.63s/config]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🏆 Best parameters: {'epochs': 20, 'min_count': 5, 'negative': 5, 'sg': 1, 'vector_size': 200, 'window': 5}\n",
      "📈 Best MAP Score: 0.0259\n",
      "💯 Best Similarity Score; 0.3135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# --- Define parameter space ---\n",
    "# 3 × 3 × 3 × 2 × 2 × 2 = 216 configurations\n",
    "param_grid = {\n",
    "    'vector_size': [100, 200, 300],\n",
    "    'window': [5, 8, 12],\n",
    "    'negative': [5, 10, 15],\n",
    "    'epochs': [10, 20],\n",
    "    'min_count': [2, 5],\n",
    "    'sg': [0, 1]\n",
    "}\n",
    "\n",
    "# --- Get a random subset of combinations ---\n",
    "all_combinations = list(ParameterGrid(param_grid))\n",
    "subset_size = 30  # Change this to control how many configs to try\n",
    "selected_combinations = random.sample(all_combinations, subset_size)\n",
    "\n",
    "# --- Run evaluation ---\n",
    "best = {'score': -1}\n",
    "log = []\n",
    "for params in tqdm(selected_combinations, desc=\"Tuning Word2Vec\", unit=\"config\"):\n",
    "    model = Word2Vec(processed_sentences, **params, sample=1e-4, workers=6, seed=42)\n",
    "    score = evaluate_query_expansion(model, validation_queries)\n",
    "    sim = average_similarity_top_words(model, top_50_words)\n",
    "    log.append({'params': params, 'score': score, 'sim': sim, 'model': model}) # To call this\n",
    "    if score > best['score']:\n",
    "        best = {'params': params, 'score': score, 'sim': sim, 'model': model}\n",
    "best_model = best['model']\n",
    "print(f\"\\n🏆 Best parameters (for MAP): {best['params']}\")\n",
    "print(f\"📈 Best Average MAP Score: {best['score']:.4f}\")\n",
    "print(f\"💯 Average Similarity Score: {best['sim']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word2Vec model saved (only KeyedVectors) to: ../models/best_w2v_ir2025.kv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# --- Save model ---\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save only the KeyedVectors part\n",
    "best_model.wv.save(\"../models/best_w2v_ir2025.kv\")\n",
    "print(\"✅ Word2Vec model saved (only KeyedVectors) to: ../models/best_w2v_ir2025.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Word2Vec model (only KeyedVectors) successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# --- Load model ---\n",
    "kv_model = KeyedVectors.load(\"../models/best_w2v_ir2025.kv\", mmap='r')\n",
    "print(\"✅ Word2Vec model (only KeyedVectors) successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 66705\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(kv_model.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean vector norm: 3.8426\n",
      "Max vector norm: 8.3411\n",
      "Min vector norm: 0.0397\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_norms = np.linalg.norm(kv_model.vectors, axis=1)\n",
    "print(f\"Mean vector norm: {np.mean(vector_norms):.4f}\")\n",
    "print(f\"Max vector norm: {np.max(vector_norms):.4f}\")\n",
    "print(f\"Min vector norm: {np.min(vector_norms):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:26<00:00,  1.88query/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "print(\"Expanding Queries..\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_word2vec(query[\"text\"], kv_model)\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to ../data/trec-covid/queries_expanded_word2vec.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_word2vec.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write(q)\n",
    "    print(\"✅ Expanded queries saved to ../data/trec-covid/queries_expanded_word2vec.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 20:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 20: 100%|██████████| 50/50 [00:00<00:00, 52.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_3/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 30: 100%|██████████| 50/50 [00:00<00:00, 73.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_3/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Word2Vec for run with k = 50: 100%|██████████| 50/50 [00:00<00:00, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_3/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_3(expanded_queries_path):\n",
    "    # Load queries\n",
    "    with open(expanded_queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_3\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Expanded Queries with Word2Vec for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"expanded_text\"] # already did this: expand_query_with_word2vec()\n",
    "            \n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": { \"text\": query_text }},\n",
    "                size=k\n",
    "            )\n",
    "\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_3/retrieval_top_{k}.json\")\n",
    "\n",
    "    return runs\n",
    "    \n",
    "runs = process_queries_phase_3(\"../data/trec-covid/queries_expanded_word2vec.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_3\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_3\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_3\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_3\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "            # num_docs = len(docs)\n",
    "            # print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "        \n",
    "compute_metrics(qrels, runs, \"phase_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare (e.g. [20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (e.g. ['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 1 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 3 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 3 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 3 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 3 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 3 avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9469963d-0dc3-4fd1-b536-5721e181c2a0",
       "rows": [
        [
         "20",
         "0.020569367599275835",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5479999999999999",
         "0.020552216401656397",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.021517297872021928",
         "0.6520000000000001",
         "0.5999999999999999",
         "0.5746666666666667",
         "0.5549999999999999"
        ],
        [
         "30",
         "0.027752701467553504",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.028368990673951277",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.02866090487697676",
         "0.6520000000000001",
         "0.5999999999999999",
         "0.5746666666666667",
         "0.5559999999999998"
        ],
        [
         "50",
         "0.039910502219297476",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.04085611017398103",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.040929728681477984",
         "0.6520000000000001",
         "0.5999999999999999",
         "0.5746666666666667",
         "0.5559999999999998"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 MAP</th>\n",
       "      <th>Phase 2 avgPre@5</th>\n",
       "      <th>Phase 2 avgPre@10</th>\n",
       "      <th>Phase 2 avgPre@15</th>\n",
       "      <th>Phase 2 avgPre@20</th>\n",
       "      <th>Phase 3 MAP</th>\n",
       "      <th>Phase 3 avgPre@5</th>\n",
       "      <th>Phase 3 avgPre@10</th>\n",
       "      <th>Phase 3 avgPre@15</th>\n",
       "      <th>Phase 3 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.020552</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.021517</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.574667</td>\n",
       "      <td>0.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.028369</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.028661</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.574667</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.040856</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.040930</td>\n",
       "      <td>0.652</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.574667</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.020569              0.64              0.582              0.564   \n",
       "30     0.027753              0.64              0.582              0.564   \n",
       "50     0.039911              0.64              0.582              0.564   \n",
       "\n",
       "    Phase 1 avgPre@20  Phase 2 MAP  Phase 2 avgPre@5  Phase 2 avgPre@10  \\\n",
       "k                                                                         \n",
       "20              0.548     0.020552             0.608              0.586   \n",
       "30              0.549     0.028369             0.608              0.586   \n",
       "50              0.549     0.040856             0.608              0.586   \n",
       "\n",
       "    Phase 2 avgPre@15  Phase 2 avgPre@20  Phase 3 MAP  Phase 3 avgPre@5  \\\n",
       "k                                                                         \n",
       "20              0.556              0.538     0.021517             0.652   \n",
       "30              0.556              0.538     0.028661             0.652   \n",
       "50              0.556              0.538     0.040930             0.652   \n",
       "\n",
       "    Phase 3 avgPre@10  Phase 3 avgPre@15  Phase 3 avgPre@20  \n",
       "k                                                            \n",
       "20                0.6           0.574667              0.555  \n",
       "30                0.6           0.574667              0.556  \n",
       "50                0.6           0.574667              0.556  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    \"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    # \"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}\n",
    "_ = compare_phases(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
