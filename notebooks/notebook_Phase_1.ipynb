{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Phase 1: Classical Information Retrieval on the IR2025 Collection\n",
    "\n",
    "\n",
    "In this project, we build and evaluate a **BM25‐based Elasticsearch index** over the **IR2025** corpus as a baseline before any query expansion.\n",
    "\n",
    "---\n",
    "> Maria Schoinaki, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210191@aueb.gr <br/><br/>\n",
    "\n",
    "> Nikos Mitsakis, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210122@aueb.gr <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this notebook, we will:_\n",
    "1. Configure and create an Elasticsearch index with a custom analyzer  \n",
    "2. Ingest the IR2025 documents in bulk  \n",
    "3. Execute top-k retrieval (k=20,30,50) for each test query  \n",
    "4. Evaluate using MAP@k and Precision@k (k=5,10,15,20) via pytrec_eval  \n",
    "5. Summarize and analyze our results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"..\\\\requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports  \n",
    "\n",
    "**3210122 + 3210191 = 6420313**\n",
    "- So we get the `trec_covid` IR2025 collection.\n",
    "\n",
    "Here we import all necessary libraries, set up environment variables, and instantiate the Elasticsearch client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass))\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzer & Index Creation  \n",
    "We define a **custom English analyzer** (standard tokenizer + lowercase + stopword removal + Porter stemming) and create the Elasticsearch index with BM25 similarity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'ir2025-index' deleted.\n",
      "✅ Index 'ir2025-index' created\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"✅ Index '{INDEX_NAME}' deleted.\")\n",
    "\n",
    "# Define the settings and mappings for the index\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"english_stop\": {\n",
    "                \"type\": \"stop\",\n",
    "                \"stopwords\": \"_english_\"\n",
    "            },\n",
    "            \"english_stemmer\": {\n",
    "                \"type\": \"kstem\"\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"custom_english\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\", # Converts all terms to lowercase\n",
    "                    \"english_stop\", # Removes English stop words\n",
    "                    \"english_stemmer\" # Reduces words to their root form usign kstem\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"doc_id\": {\"type\": \"keyword\"},\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_english\",\n",
    "            \"similarity\": \"BM25\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index with the specified settings and mappings\n",
    "es.indices.create(\n",
    "    index=INDEX_NAME,\n",
    "    settings=settings,\n",
    "    mappings=mappings\n",
    ")\n",
    "print(f\"✅ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion  \n",
    "Using the `streaming_bulk` helper, we ingest all IR2025 documents in chunks of 500.  \n",
    "A progress bar (tqdm) provides real‐time feedback on indexing throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171332/171332 [00:53<00:00, 3191.38docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 171332/171332 documents into 'ir2025-index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import streaming_bulk\n",
    "\n",
    "# Generator function to yield documents\n",
    "def generate_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            yield {\n",
    "                \"_index\": INDEX_NAME,\n",
    "                \"_id\": doc[\"_id\"],\n",
    "                \"_source\": {\n",
    "                    \"doc_id\": doc[\"_id\"],\n",
    "                    \"text\": doc[\"text\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Path to the JSONL file\n",
    "file_path = \"../data/trec-covid/corpus.jsonl\"\n",
    "\n",
    "# Count the total number of documents for the progress bar\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    total_docs = sum(1 for _ in f)\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress = tqdm(unit=\"docs\", total=total_docs)\n",
    "\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=es, actions=generate_documents(file_path), chunk_size=500):\n",
    "    progress.update(1)\n",
    "    successes += int(ok)\n",
    "\n",
    "progress.close()\n",
    "print(f\"✅ Indexed {successes}/{total_docs} documents into '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Query Execution  \n",
    "For each query in `queries.jsonl`, we issue a `match` query on the `text` field and collect the top-k document IDs for k=20,30,50.  \n",
    "Results are saved as JSON mappings: `{ query_id: [doc1, doc2, …] }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 20: 100%|██████████| 50/50 [00:02<00:00, 16.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 30: 100%|██████████| 50/50 [00:00<00:00, 75.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 50: 100%|██████████| 50/50 [00:00<00:00, 56.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_1(queries_path):\n",
    "    # Load queries\n",
    "    with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "        print(f\"Loaded {len(queries)} queries.\")\n",
    "        \n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50] # Number of top documents to retrieve\n",
    "        \n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        # Prepare output directory\n",
    "        output_dir = f\"../results/phase_1\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for query in tqdm(queries, desc=f\"Processing Queries for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"text\"]\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": {\"text\": query_text}},\n",
    "                size=k\n",
    "            )\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "                \n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_1/retrieval_top_{k}.json\")\n",
    "    \n",
    "    return runs\n",
    "    \n",
    "runs = process_queries_phase_1(\"../data/trec-covid/queries.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Qrels & Evaluation  \n",
    "1. **Load** relevance judgments (qrels) from the TSV file into a Python dict.  \n",
    "2. **Compute** MAP@k and Precision@k (5,10,15,20) using `pytrec_eval`.  \n",
    "3. **Cross‐validate** with the `trec_eval` binary to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rprec', 'infAP', 'bpref', 'relative_P', 'recip_rank', 'gm_map', 'runid', 'num_nonrel_judged_ret', 'set_relative_P', 'utility', 'Rprec_mult', 'num_ret', 'iprec_at_recall', 'success', 'num_rel_ret', 'P', 'gm_bpref', 'Rndcg', 'ndcg_cut', 'set_map', 'recall', 'set_P', 'binG', 'ndcg_rel', 'set_recall', 'num_rel', 'G', 'set_F', 'map_cut', 'map', 'relstring', '11pt_avg', 'ndcg', 'num_q'}\n"
     ]
    }
   ],
   "source": [
    "print(pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "            # num_docs = len(docs)\n",
    "            # print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "        \n",
    "compute_metrics(qrels, runs, \"phase_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare (e.g. [20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (e.g. ['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.020569              0.64              0.582              0.564   \n",
       "30     0.027753              0.64              0.582              0.564   \n",
       "50     0.039911              0.64              0.582              0.564   \n",
       "\n",
       "    Phase 1 avgPre@20  \n",
       "k                      \n",
       "20              0.548  \n",
       "30              0.549  \n",
       "50              0.549  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    #\"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    #\"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    #\"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}\n",
    "_ = compare_phases(phases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
