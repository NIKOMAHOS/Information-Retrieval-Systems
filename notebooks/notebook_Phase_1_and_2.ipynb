{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"..\\\\requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3210122 + 3210191 = 6420313\n",
    "- So we get the `trec_covid` IR2025 collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load and Preprocess the Data: \n",
    "> \n",
    "> Manual Code is not used as it is done by ElasticSearch when we load the data as we have defined the settings and mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem import SnowballStemmer\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# def preprocess(text):\n",
    "#     # Lowercase\n",
    "#     text = text.lower()\n",
    "#     # Remove punctuation\n",
    "#     text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "#     # Tokenize\n",
    "#     tokens = text.split()\n",
    "#     # Remove stopwords and apply stemming\n",
    "#     tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "#     # Join back into string\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# def process_jsonl(input_path=\"..\\\\data\\\\trec-covid\\\\corpus.jsonl\", output_path=\"..\\\\data\\\\corpus_processed.jsonl\"):\n",
    "#     with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "#         for line in infile:\n",
    "#             obj = json.loads(line)\n",
    "#             if \"text\" in obj:\n",
    "#                 obj[\"text\"] = preprocess(obj[\"text\"])\n",
    "#             json.dump(obj, outfile)\n",
    "#             outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verify preprocessing works\n",
    "# example = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
    "# print(preprocess(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_jsonl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load, Preprocess Data & Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass))\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'ir2025-index' deleted.\n",
      "✅ Index 'ir2025-index' created\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"✅ Index '{INDEX_NAME}' deleted.\")\n",
    "\n",
    "# Define the settings and mappings for the index\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"english_stop\": {\n",
    "                \"type\": \"stop\",\n",
    "                \"stopwords\": \"_english_\"\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"custom_english\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\",\n",
    "                    \"english_stop\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"doc_id\": {\"type\": \"keyword\"},\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_english\",\n",
    "            \"similarity\": \"BM25\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index with the specified settings and mappings\n",
    "es.indices.create(\n",
    "    index=INDEX_NAME,\n",
    "    settings=settings,\n",
    "    mappings=mappings\n",
    ")\n",
    "print(f\"✅ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Populate Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171332/171332 [00:40<00:00, 4225.04docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 171332/171332 documents into 'ir2025-index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import streaming_bulk\n",
    "\n",
    "# Generator function to yield documents\n",
    "def generate_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            yield {\n",
    "                \"_index\": INDEX_NAME,\n",
    "                \"_id\": doc[\"_id\"],\n",
    "                \"_source\": {\n",
    "                    \"doc_id\": doc[\"_id\"],\n",
    "                    \"text\": doc[\"text\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Path to the JSONL file\n",
    "file_path = \"../data/trec-covid/corpus.jsonl\"\n",
    "\n",
    "# Count the total number of documents for the progress bar\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    total_docs = sum(1 for _ in f)\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress = tqdm(unit=\"docs\", total=total_docs)\n",
    "\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=es, actions=generate_documents(file_path), chunk_size=500):\n",
    "    progress.update(1)\n",
    "    successes += int(ok)\n",
    "\n",
    "progress.close()\n",
    "print(f\"✅ Indexed {successes}/{total_docs} documents into '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Execute Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 20: 100%|██████████| 50/50 [00:02<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 30: 100%|██████████| 50/50 [00:00<00:00, 72.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 50: 100%|██████████| 50/50 [00:00<00:00, 54.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_1/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_1(queries_path):\n",
    "    # Load queries\n",
    "    with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "        \n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50] # Number of top documents to retrieve\n",
    "        \n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        # Prepare output directory\n",
    "        output_dir = f\"../results/phase_1\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        for query in tqdm(queries, desc=f\"Processing Queries for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"text\"]\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": {\"text\": query_text}},\n",
    "                size=k\n",
    "            )\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "                \n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_1/retrieval_top_{k}.json\")\n",
    "    \n",
    "    return runs\n",
    "    \n",
    "runs = process_queries_phase_1(\"../data/trec-covid/queries.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Query Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11pt_avg', 'utility', 'Rndcg', 'gm_bpref', 'recip_rank', 'relative_P', 'infAP', 'Rprec', 'bpref', 'map_cut', 'P', 'set_P', 'map', 'set_map', 'num_q', 'set_relative_P', 'ndcg', 'gm_map', 'ndcg_rel', 'runid', 'iprec_at_recall', 'Rprec_mult', 'relstring', 'num_ret', 'recall', 'success', 'ndcg_cut', 'num_nonrel_judged_ret', 'binG', 'num_rel', 'set_recall', 'set_F', 'G', 'num_rel_ret'}\n"
     ]
    }
   ],
   "source": [
    "print(pytrec_eval.supported_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_1\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_1\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder):    \n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P.5', 'P.10', 'P.15', 'P.20'})\n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        results = evaluator.evaluate(run)\n",
    "    \n",
    "        # Compute average metrics\n",
    "        metrics = ['map', 'P_5', 'P_10', 'P_15', 'P_20']\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "        \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "    \n",
    "compute_metrics(qrels, runs, 'phase_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Query Expansion with Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/trec-covid/'\n",
    "\n",
    "with jsonlines.open(input_dir + 'corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]\n",
    "\n",
    "with jsonlines.open(input_dir + 'queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "\n",
    "test_df = pd.read_csv(input_dir + 'qrels/' + 'test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_synonyms(word, max_synonyms=1):\n",
    "    synonyms = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        # Keep only nouns and adjectives\n",
    "        if syn.pos() in ('n', 'a'):\n",
    "            for lemma in syn.lemmas():\n",
    "                name = lemma.name().replace(\"_\", \" \").lower()\n",
    "\n",
    "                # Filter out:\n",
    "                if name == word: # Skip the original word\n",
    "                    continue\n",
    "                if len(name.split()) > 1:  # Skip multi-word phrases\n",
    "                    continue\n",
    "                if not name.isalpha(): # Skip non-alphabetic words\n",
    "                    continue\n",
    "\n",
    "                synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency (most-used synonyms first)\n",
    "    ranked_synonyms = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "\n",
    "    return ranked_synonyms[:max_synonyms] # Return up to max_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_query_with_synonyms(query_text):\n",
    "    expanded_terms = []\n",
    "    for word in word_tokenize(query_text.lower()):\n",
    "        if word.isalpha() and word not in stop_words:\n",
    "            synonyms = get_wordnet_synonyms(word)\n",
    "            expanded_terms.extend(synonyms)\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 320.00it/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "for query in tqdm(queries):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_synonyms(query[\"text\"])\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to queries_expanded_wordnet.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_wordnet.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write({\n",
    "            \"_id\": q[\"_id\"],\n",
    "            \"text\": q[\"expanded_text\"]\n",
    "        })\n",
    "    print(\"✅ Expanded queries saved to queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries_phase_2(queries_path):\n",
    "    # Load queries\n",
    "    with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Queries for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            original_text = query[\"text\"]\n",
    "            query_text = expand_query_with_synonyms(original_text)\n",
    "            \n",
    "            # This tells ElasticSearch:\n",
    "            # - “Give higher importance to the original query terms (boost=2)”\n",
    "            # - “But also consider the synonyms (with normal weight)”\n",
    "            # => This prevents the expanded terms from drowning out the original intent.\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            { \"match\": { \"text\": { \"query\": original_text, \"boost\": 3 } }},\n",
    "                            { \"match\": { \"text\": query_text }}\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                size=k\n",
    "            )\n",
    "\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_2/retrieval_top_{k}.json\")\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 20:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 20: 100%|██████████| 50/50 [00:01<00:00, 27.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 30: 100%|██████████| 50/50 [00:01<00:00, 26.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 50: 100%|██████████| 50/50 [00:02<00:00, 23.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "runs = process_queries_phase_2(\"../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(qrels, runs, 'phase_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the (AVG) results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    # \"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    # \"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare (e.g. [20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (e.g. ['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics:\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 1 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@p",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@p",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "17368173-6c15-473b-9a2e-917de4fd4258",
       "rows": [
        [
         "20",
         "0.019879572677609786",
         "0.019879572677609786",
         "0.632",
         "0.5899999999999999",
         "0.5560000000000002",
         "0.5449999999999999",
         "0.015924723596881855",
         "0.015924723596881855",
         "0.5039999999999999",
         "0.5100000000000001",
         "0.47866666666666674",
         "0.4640000000000001"
        ],
        [
         "30",
         "0.026913288876425953",
         "0.026913288876425953",
         "0.0",
         "0.5899999999999999",
         "0.0",
         "0.0",
         "0.021949052246424464",
         "0.021949052246424464",
         "0.0",
         "0.5100000000000001",
         "0.0",
         "0.0"
        ],
        [
         "50",
         "0.03878957151816462",
         "0.03878957151816462",
         "0.0",
         "0.5899999999999999",
         "0.0",
         "0.0",
         "0.03174958989844058",
         "0.03174958989844058",
         "0.0",
         "0.5100000000000001",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@p</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 MAP</th>\n",
       "      <th>Phase 2 avgPre@p</th>\n",
       "      <th>Phase 2 avgPre@5</th>\n",
       "      <th>Phase 2 avgPre@10</th>\n",
       "      <th>Phase 2 avgPre@15</th>\n",
       "      <th>Phase 2 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.015925</td>\n",
       "      <td>0.504</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.478667</td>\n",
       "      <td>0.464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021949</td>\n",
       "      <td>0.021949</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.031750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@p  Phase 1 avgPre@5  Phase 1 avgPre@10  \\\n",
       "k                                                                        \n",
       "20     0.019880          0.019880             0.632               0.59   \n",
       "30     0.026913          0.026913             0.000               0.59   \n",
       "50     0.038790          0.038790             0.000               0.59   \n",
       "\n",
       "    Phase 1 avgPre@15  Phase 1 avgPre@20  Phase 2 MAP  Phase 2 avgPre@p  \\\n",
       "k                                                                         \n",
       "20              0.556              0.545     0.015925          0.015925   \n",
       "30              0.000              0.000     0.021949          0.021949   \n",
       "50              0.000              0.000     0.031750          0.031750   \n",
       "\n",
       "    Phase 2 avgPre@5  Phase 2 avgPre@10  Phase 2 avgPre@15  Phase 2 avgPre@20  \n",
       "k                                                                              \n",
       "20             0.504               0.51           0.478667              0.464  \n",
       "30             0.000               0.51           0.000000              0.000  \n",
       "50             0.000               0.51           0.000000              0.000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = compare_phases(phases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
