{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Query Expansion with Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"..\\\\requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass))\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'ir2025-index' already exists.\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    print(f\"✅ Index '{INDEX_NAME}' already exists.\")\n",
    "\n",
    "else:\n",
    "    # Define the settings and mappings for the index\n",
    "    settings = {\n",
    "        \"analysis\": {\n",
    "            \"filter\": {\n",
    "                \"english_stop\": {\n",
    "                    \"type\": \"stop\",\n",
    "                    \"stopwords\": \"_english_\"\n",
    "                },\n",
    "                \"english_stemmer\": {\n",
    "                    \"type\": \"kstem\"\n",
    "                }\n",
    "            },\n",
    "            \"analyzer\": {\n",
    "                \"custom_english\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"lowercase\", # Converts all terms to lowercase\n",
    "                        \"english_stop\", # Removes English stop words\n",
    "                        \"english_stemmer\" # Reduces words to their root form usign kstem\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    mappings = {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"keyword\"},\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"custom_english\",\n",
    "                \"similarity\": \"BM25\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create the index with the specified settings and mappings\n",
    "    es.indices.create(\n",
    "        index=INDEX_NAME,\n",
    "        settings=settings,\n",
    "        mappings=mappings\n",
    "    )\n",
    "    print(f\"✅ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab') # instead of punkt: NLTK > 3.8.2 !\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/trec-covid/'\n",
    "\n",
    "with jsonlines.open(input_dir + 'corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate custom_english Analyzer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # KrovetzStemmer supports up to python 3.10 at best \n",
    "import string\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer() # It's \"Closer\" to Korvetz than Snowball is\n",
    "\n",
    "def es_like_preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower().strip()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply stemming (Porter)\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words or not token.isalpha()]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_and_save_tfidf_model(corpus, output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Build TF-IDF model from corpus, compute statistics, and save the model.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of documents with 'text' field\n",
    "        output_dir: Directory to save models and statistics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics_dict)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize counters for statistics\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "    statistics = {}\n",
    "\n",
    "    # Preprocess with detailed statistics\n",
    "    print(\"Preprocessing corpus...\")\n",
    "    preprocessed_corpus = []\n",
    "    for doc in tqdm(corpus, desc=\"Preprocessing documents\", unit=\"doc\"):\n",
    "        processed_text = es_like_preprocess(doc[\"text\"])\n",
    "        tokens = processed_text.split()\n",
    "        \n",
    "        # Update statistics\n",
    "        total_tokens += len(tokens)\n",
    "        unique_tokens.update(tokens)\n",
    "        \n",
    "        preprocessed_corpus.append(processed_text)\n",
    "\n",
    "    # Save preprocessing statistics\n",
    "    statistics['preprocessing'] = {\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': len(unique_tokens),\n",
    "        'average_tokens_per_doc': total_tokens/len(corpus)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nPreprocessing statistics:\")\n",
    "    print(f\"- Total tokens: {total_tokens:,}\")\n",
    "    print(f\"- Unique tokens: {len(unique_tokens):,}\")\n",
    "    print(f\"- Average tokens per document: {total_tokens/len(corpus):,.1f}\")\n",
    "\n",
    "    # Build TF-IDF model with detailed progress\n",
    "    print(\"\\nBuilding TF-IDF model...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "    with tqdm(total=3, desc=\"TF-IDF computation\") as pbar:\n",
    "        # Fit the vectorizer\n",
    "        tfidf_vectorizer.fit(preprocessed_corpus)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        idf_scores = dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Calculate and save TF-IDF statistics\n",
    "    idf_values = list(idf_scores.values())\n",
    "    statistics['tfidf'] = {\n",
    "        'vocabulary_size': len(idf_scores),\n",
    "        'average_idf': float(np.mean(idf_values)),\n",
    "        'max_idf': float(max(idf_values)),\n",
    "        'min_idf': float(min(idf_values))\n",
    "    }\n",
    "\n",
    "    print(\"\\nTF-IDF statistics:\")\n",
    "    print(f\"- Vocabulary size: {len(idf_scores):,} terms\")\n",
    "    print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "    print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "    print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    "\n",
    "    # Save everything\n",
    "    print(\"\\nSaving models and statistics...\")\n",
    "    try:\n",
    "        # Save vectorizer\n",
    "        joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Save IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(idf_scores, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Save statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(statistics, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(\"\\n✅ Saved successfully:\")\n",
    "        print(f\"- Vectorizer: {os.path.join(output_dir, 'tfidf_vectorizer.joblib')}\")\n",
    "        print(f\"- IDF scores: {os.path.join(output_dir, 'idf_scores.json')}\")\n",
    "        print(f\"- Statistics: {os.path.join(output_dir, 'tfidf_statistics.json')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error saving files: {e}\")\n",
    "        \n",
    "    return tfidf_vectorizer, idf_scores\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_tfidf_model(output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Load the saved TF-IDF model, IDF scores, and statistics.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory where models and statistics are saved\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorizer\n",
    "        vectorizer = joblib.load(os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Load IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'r', encoding='utf-8') as f:\n",
    "            idf_scores = json.load(f)\n",
    "            \n",
    "        # Load statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'r', encoding='utf-8') as f:\n",
    "            statistics = json.load(f)\n",
    "        \n",
    "        print(\"\\nModel validation:\")\n",
    "        print(f\"- Vocabulary size: {len(idf_scores):,}\")\n",
    "        print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "        print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "        print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    " \n",
    "        print(\"✅ Model loaded successfully\")\n",
    "        return vectorizer, idf_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None, None\n",
    "        \n",
    "# Transform new text using the loaded vectorizer\n",
    "def transform_text(text, vectorizer):\n",
    "    \"\"\"Transform new text using the loaded vectorizer\"\"\"\n",
    "    try:\n",
    "        transformed = vectorizer.transform([text])\n",
    "        return transformed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model validation:\n",
      "- Vocabulary size: 298,422\n",
      "- Average IDF: 11.84\n",
      "- Max IDF: 12.36\n",
      "- Min IDF: 2.03\n",
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Build and save the model\n",
    "vectorizer, idf_scores = build_and_save_tfidf_model(corpus)\n",
    " \n",
    "# Test loading\n",
    "vectorizer, idf_scores = load_tfidf_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_wordnet_synonyms(word, max_synonyms=3):\n",
    "    synonyms = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace(\"_\", \" \").lower()\n",
    "\n",
    "            # Filter out:\n",
    "            if name == word: # Skip the original word\n",
    "                continue\n",
    "            if len(name.split()) > 1:  # Skip multi-word phrases\n",
    "                continue\n",
    "            if not name.isalpha(): # Skip non-alphabetic words\n",
    "                continue\n",
    "\n",
    "            synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency (most-used synonyms first)\n",
    "    ranked_synonyms = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "\n",
    "    return ranked_synonyms[:max_synonyms] # Return up to max_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def is_expandable(pos):\n",
    "    return pos.startswith('NN') or pos.startswith('JJ')  # nouns & adjectives\n",
    "    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_query_with_wordnet(query_text, tfidf_vectorizer, max_synonyms=1, n_expand=1):\n",
    "    # For expansion decisions, work with the original query text\n",
    "    tokens = word_tokenize(query_text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Candidate words = noun/adjective, not stopword, alphabetic\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "\n",
    "    # Get IDF scores from vectorizer to identify important terms\n",
    "    # We need to preprocess the word to match vectorizer's vocabulary\n",
    "    idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    # Score each candidate by IDF from the corpus\n",
    "    scored = [\n",
    "        (word, idf_scores.get(es_like_preprocess(word), 0.0))  # preprocess just for vocabulary lookup\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    # Sort by IDF weight descending and keep top-n\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "    \n",
    "    # Expand only top words\n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        synonyms = get_wordnet_synonyms(word, max_synonyms)\n",
    "        if synonyms:  # only add if we found synonyms\n",
    "            expanded_terms.extend(synonyms)\n",
    "\n",
    "    # Return original query + expansion terms (let Elasticsearch handle preprocessing)\n",
    "    if expanded_terms:\n",
    "        return query_text + \" \" + \" \".join(expanded_terms)\n",
    "        \n",
    "    return query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 queries.\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(input_dir + 'queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "    print(f\"Loaded {len(queries)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:29<00:00,  1.68query/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "print(\"Expanding Queries..\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_wordnet(query[\"text\"], vectorizer)\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_wordnet.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write(q)\n",
    "    print(\"✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 20: 100%|██████████| 50/50 [00:01<00:00, 27.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 30: 100%|██████████| 50/50 [00:01<00:00, 43.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 50: 100%|██████████| 50/50 [00:01<00:00, 35.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_2(expanded_queries_path):\n",
    "    # Load queries\n",
    "    with open(expanded_queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Expanded Queries with WordNet for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"expanded_text\"] # already did this: expand_query_with_wordnet()\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={{ \"match\": { \"text\": query_text}}},\n",
    "                size=k\n",
    "            )\n",
    "\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_2/retrieval_top_{k}.json\")\n",
    "\n",
    "    return runs\n",
    "    \n",
    "runs = process_queries_phase_2(\"../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "            # num_docs = len(docs)\n",
    "            # print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "        \n",
    "compute_metrics(qrels, runs, 'phase_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the (AVG) results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare ([20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 MAP</th>\n",
       "      <th>Phase 2 avgPre@5</th>\n",
       "      <th>Phase 2 avgPre@10</th>\n",
       "      <th>Phase 2 avgPre@15</th>\n",
       "      <th>Phase 2 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.020473</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.028280</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.554667</td>\n",
       "      <td>0.536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  ...  Phase 2 avgPre@15  Phase 2 avgPre@20\n",
       "k                                  ...                                      \n",
       "20     0.020569              0.64  ...           0.554667              0.536\n",
       "30     0.027753              0.64  ...           0.554667              0.536\n",
       "50     0.039911              0.64  ...           0.554667              0.536\n",
       "\n",
       "[3 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    # \"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    # \"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}\n",
    "_ = compare_phases(phases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
