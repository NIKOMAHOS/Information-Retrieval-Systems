{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval - Phase 2: Query Expansion with WordNet Synonyms on the IR2025 Collection\n",
    "\n",
    "\n",
    "In this project, we expand IR2025 queries using **WordNet synonyms** (via **NLTK**), re-run our **BM25 Elasticsearch** retrieval, and evaluate the impact of synonym-based query expansion.\n",
    "\n",
    "---\n",
    "> Maria Schoinaki, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210191@aueb.gr <br/><br/>\n",
    "\n",
    "> Nikos Mitsakis, BSc Student <br />\n",
    "> Department of Informatics, Athens University of Economics and Business <br />\n",
    "> p3210122@aueb.gr <br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In this notebook, we will:_\n",
    "\n",
    "1. Set up the Python environment and Elasticsearch connection  \n",
    "2. Load original queries, relevance judgments (qrels), and prepare the TF–IDF model  \n",
    "3. Define POS-mapping and synonym-extraction functions using NLTK’s WordNet API  \n",
    "4. Generate **Scenario A** expanded queries (all single-word synonyms) and write them to `queries_expanded_wordnet.jsonl`  \n",
    "5. Generate **Scenario B** expanded queries (hypernym-only) and write them to `queries_expanded_hypernym.jsonl`  \n",
    "6. Execute top-k retrieval (k = 20, 30, 50) for both Scenario A and Scenario B  \n",
    "7. Evaluate retrieval performance with MAP@k and P@k (k = 5, 10, 15, 20) via `pytrec_eval` for each scenario  \n",
    "8. Compare Phase 2 results (both scenarios) against the Phase 1 BM25 baseline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qq -r \"..\\\\requirements.txt\" \n",
    "# fix path accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports  \n",
    "\n",
    "**3210122 + 3210191 = 6420313**\n",
    "- So we get the `trec_covid` IR2025 collection.\n",
    "\n",
    "Here we import all necessary libraries, set up environment variables, and instantiate the Elasticsearch client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass), request_timeout=30, retry_on_timeout=True, max_retries=10)\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzer & Index Creation  \n",
    "We define a **custom English analyzer** (standard tokenizer + lowercase + stopword removal + Krovetz stemming) and create the Elasticsearch index with BM25 similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Index 'ir2025-index' deleted\n",
      "✅ Index 'ir2025-index' created\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"ir2025-index\"\n",
    "\n",
    "# Delete the index if it already exists\n",
    "if es.indices.exists(index=INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    print(f\"✅ Index '{INDEX_NAME}' deleted\")\n",
    "\n",
    "# Define the settings and mappings for the index\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"english_stop\": {\n",
    "                \"type\": \"stop\",\n",
    "                \"stopwords\": \"_english_\"\n",
    "            },\n",
    "            \"english_stemmer\": {\n",
    "                \"type\": \"kstem\"\n",
    "            }\n",
    "        },\n",
    "        \"analyzer\": {\n",
    "            \"custom_english\": {\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"lowercase\", # Converts all terms to lowercase\n",
    "                    \"english_stop\", # Removes English stop words\n",
    "                    \"english_stemmer\" # Reduces words to their root form usign kstem\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"doc_id\": {\"type\": \"keyword\"},\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"custom_english\",\n",
    "            \"similarity\": \"BM25\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the index with the specified settings and mappings\n",
    "es.indices.create(\n",
    "    index=INDEX_NAME,\n",
    "    settings=settings,\n",
    "    mappings=mappings\n",
    ")\n",
    "print(f\"✅ Index '{INDEX_NAME}' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Ingestion  \n",
    "Using the `streaming_bulk` helper, we ingest all IR2025 documents in chunks of 500.  \n",
    "A progress bar (tqdm) provides real‐time feedback on indexing throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171332/171332 [00:47<00:00, 3624.57docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed 171332/171332 documents into 'ir2025-index'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch.helpers import streaming_bulk\n",
    "\n",
    "# Generator function to yield documents\n",
    "def generate_documents(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            yield {\n",
    "                \"_index\": INDEX_NAME,\n",
    "                \"_id\": doc[\"_id\"],\n",
    "                \"_source\": {\n",
    "                    \"doc_id\": doc[\"_id\"],\n",
    "                    \"text\": doc[\"text\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "# Count the total number of documents for the progress bar\n",
    "with open(\"../data/trec-covid/corpus.jsonl\", 'r', encoding='utf-8') as f:\n",
    "    total_docs = sum(1 for _ in f)\n",
    "\n",
    "# Initialize the progress bar\n",
    "progress = tqdm(unit=\"docs\", total=total_docs)\n",
    "\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(client=es, actions=generate_documents(\"../data/trec-covid/corpus.jsonl\"), chunk_size=500):\n",
    "    progress.update(1)\n",
    "    successes += int(ok)\n",
    "\n",
    "progress.close()\n",
    "print(f\"✅ Indexed {successes}/{total_docs} documents into '{INDEX_NAME}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. NLTK Setup and Corpus Preprocessing\n",
    "\n",
    "Download required NLTK data, load the IR2025 corpus into memory, and define a Python function to simulate the `custom_english` analyzer for downstream TF–IDF modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the necessary NLTK corpora and models.\n",
    "\n",
    "- **Tokenization & POS Tagging**: `punkt_tab`, `averaged_perceptron_tagger`  \n",
    "- **Stopword List**: `stopwords`  \n",
    "- **Lexical Database**: `wordnet` and the multilingual WordNet (`omw-1.4`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the IR2025 corpus into memory as a list of JSON objects using `jsonlines.open`, preparing it for TF–IDF vectorization and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../data/trec-covid/corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate `custom_english` Analyzer in Python\n",
    "\n",
    "Here we replicate our Elasticsearch `custom_english` analyzer pipeline in pure Python using NLTK:\n",
    "\n",
    "1. **Lowercase & Trim**  \n",
    "2. **Punctuation Removal**  \n",
    "3. **Tokenization** (`word_tokenize`)  \n",
    "4. **Stopword Filtering** (`stopwords.words('english')`)  \n",
    "5. **Stemming** (PorterStemmer as a proxy for Krovetz)  \n",
    "\n",
    "This function lets us preprocess text identically before building the TF–IDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate custom_english Analyzer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer # KrovetzStemmer supports up to python 3.10 at best \n",
    "import string\n",
    "\n",
    "# Initialize NLTK components\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemmer = PorterStemmer() # It's \"Closer\" to Korvetz than Snowball is\n",
    "\n",
    "def es_like_preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower().strip()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and apply stemming (Porter)\n",
    "    processed_tokens = [stemmer.stem(token) for token in tokens if token not in stop_words or not token.isalpha()]\n",
    "    # Join tokens back into a single string\n",
    "    return ' '.join(processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Query Expansion with Wordrnet synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TF–IDF Model Construction and Persistence\n",
    "\n",
    "In this section, we build a TF–IDF model over the preprocessed IR2025 corpus, compute key statistics, and save all artifacts for later use:\n",
    "\n",
    "- **Preprocessing Statistics**: total tokens, unique tokens, average tokens per document  \n",
    "- **TF–IDF Statistics**: vocabulary size, average/min/max IDF  \n",
    "- **Model Artifacts**:  \n",
    "  - `tfidf_vectorizer.joblib` (fitted `TfidfVectorizer`)  \n",
    "  - `idf_scores.json` (term → IDF)  \n",
    "  - `tfidf_statistics.json` (all collected statistics)\n",
    "\n",
    "We implement three functions:\n",
    "\n",
    "1. `build_and_save_tfidf_model(corpus, output_dir)`  \n",
    "   - Preprocesses each document, updates statistics  \n",
    "   - Fits `TfidfVectorizer`, extracts IDF scores  \n",
    "   - Saves model and statistics to disk  \n",
    "\n",
    "2. `load_tfidf_model(output_dir)`  \n",
    "   - Loads the saved vectorizer, IDF scores, and statistics  \n",
    "   - Prints summary for validation  \n",
    "\n",
    "3. `transform_text(text, vectorizer)`  \n",
    "   - Applies the loaded vectorizer to new text, returning its TF–IDF representation  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_and_save_tfidf_model(corpus, output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Build TF-IDF model from corpus, compute statistics, and save the model.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of documents with 'text' field\n",
    "        output_dir: Directory to save models and statistics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics_dict)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize counters for statistics\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "    statistics = {}\n",
    "\n",
    "    # Preprocess with detailed statistics\n",
    "    print(\"Preprocessing corpus...\")\n",
    "    preprocessed_corpus = []\n",
    "    for doc in tqdm(corpus, desc=\"Preprocessing documents\", unit=\"doc\"):\n",
    "        processed_text = es_like_preprocess(doc[\"text\"])\n",
    "        tokens = processed_text.split()\n",
    "        \n",
    "        # Update statistics\n",
    "        total_tokens += len(tokens)\n",
    "        unique_tokens.update(tokens)\n",
    "        \n",
    "        preprocessed_corpus.append(processed_text)\n",
    "\n",
    "    # Save preprocessing statistics\n",
    "    statistics['preprocessing'] = {\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': len(unique_tokens),\n",
    "        'average_tokens_per_doc': total_tokens/len(corpus)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nPreprocessing statistics:\")\n",
    "    print(f\"- Total tokens: {total_tokens:,}\")\n",
    "    print(f\"- Unique tokens: {len(unique_tokens):,}\")\n",
    "    print(f\"- Average tokens per document: {total_tokens/len(corpus):,.1f}\")\n",
    "\n",
    "    # Build TF-IDF model with detailed progress\n",
    "    print(\"\\nBuilding TF-IDF model...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "\n",
    "    with tqdm(total=3, desc=\"TF-IDF computation\") as pbar:\n",
    "        # Fit the vectorizer\n",
    "        tfidf_vectorizer.fit(preprocessed_corpus)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        idf_scores = dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Calculate and save TF-IDF statistics\n",
    "    idf_values = list(idf_scores.values())\n",
    "    statistics['tfidf'] = {\n",
    "        'vocabulary_size': len(idf_scores),\n",
    "        'average_idf': float(np.mean(idf_values)),\n",
    "        'max_idf': float(max(idf_values)),\n",
    "        'min_idf': float(min(idf_values))\n",
    "    }\n",
    "\n",
    "    print(\"\\nTF-IDF statistics:\")\n",
    "    print(f\"- Vocabulary size: {len(idf_scores):,} terms\")\n",
    "    print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "    print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "    print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    "\n",
    "    # Save everything\n",
    "    print(\"\\nSaving models and statistics...\")\n",
    "    try:\n",
    "        # Save vectorizer\n",
    "        joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Save IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(idf_scores, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Save statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(statistics, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(\"\\n✅ Saved successfully:\")\n",
    "        print(f\"- Vectorizer: {os.path.join(output_dir, 'tfidf_vectorizer.joblib')}\")\n",
    "        print(f\"- IDF scores: {os.path.join(output_dir, 'idf_scores.json')}\")\n",
    "        print(f\"- Statistics: {os.path.join(output_dir, 'tfidf_statistics.json')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error saving files: {e}\")\n",
    "        \n",
    "    return tfidf_vectorizer, idf_scores\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_tfidf_model(output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Load the saved TF-IDF model, IDF scores, and statistics.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory where models and statistics are saved\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorizer\n",
    "        vectorizer = joblib.load(os.path.join(output_dir, 'tfidf_vectorizer.joblib'))\n",
    "        \n",
    "        # Load IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores.json'), 'r', encoding='utf-8') as f:\n",
    "            idf_scores = json.load(f)\n",
    "            \n",
    "        # Load statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics.json'), 'r', encoding='utf-8') as f:\n",
    "            statistics = json.load(f)\n",
    "        \n",
    "        print(\"\\nModel validation:\")\n",
    "        print(f\"- Vocabulary size: {len(idf_scores):,}\")\n",
    "        print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "        print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "        print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    " \n",
    "        print(\"✅ Model loaded successfully\")\n",
    "        return vectorizer, idf_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "# Transform new text using the loaded vectorizer\n",
    "def transform_text(text, vectorizer):\n",
    "    \"\"\"Transform new text using the loaded vectorizer\"\"\"\n",
    "    try:\n",
    "        transformed = vectorizer.transform([text])\n",
    "        return transformed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load or Build TF–IDF Model  \n",
    "\n",
    "Attempt to load the saved TF–IDF vectorizer and IDF scores. If they are not found, build the model from the corpus and save the artifacts for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model validation:\n",
      "- Vocabulary size: 298,422\n",
      "- Average IDF: 11.84\n",
      "- Max IDF: 12.36\n",
      "- Min IDF: 2.03\n",
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Test loading\n",
    "vectorizer, idf_scores = load_tfidf_model()\n",
    "if not vectorizer and not idf_scores:\n",
    "    # Build and save the model\n",
    "    vectorizer, idf_scores = build_and_save_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. WordNet Synonym Extraction Function\n",
    "\n",
    "Define a helper function `get_wordnet_synonyms` that:\n",
    "- Queries all WordNet synsets for a given word  \n",
    "- Extracts single-word, alphabetic lemmas excluding the original term  \n",
    "- Ranks candidates by usage frequency (lemma count)  \n",
    "- Returns up to `max_synonyms` most frequent synonyms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_wordnet_synonyms(word, max_synonyms=3):\n",
    "    synonyms = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace(\"_\", \" \").lower()\n",
    "\n",
    "            # Filter out:\n",
    "            if name == word: # Skip the original word\n",
    "                continue\n",
    "            if len(name.split()) > 1:  # Skip multi-word phrases\n",
    "                continue\n",
    "            if not name.isalpha(): # Skip non-alphabetic words\n",
    "                continue\n",
    "\n",
    "            synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency (most-used synonyms first)\n",
    "    ranked_synonyms = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "\n",
    "    return ranked_synonyms[:max_synonyms] # Return up to max_synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Query Expansion with WordNet and IDF-Based Term Selection\n",
    "\n",
    "This function `expand_query_with_wordnet` performs controlled query expansion by:\n",
    "\n",
    "1. **Tokenization & POS Tagging**  \n",
    "   - Lowercases and tokenizes the input query  \n",
    "   - Tags tokens and selects only nouns (NN*) and adjectives (JJ*)  \n",
    "\n",
    "2. **Candidate Scoring by IDF**  \n",
    "   - Looks up each candidate’s IDF weight via the fitted TF–IDF vectorizer  \n",
    "   - Sorts candidates by descending IDF to prioritize rare, discriminative terms  \n",
    "   - Keeps the top `n_expand` terms for expansion  \n",
    "\n",
    "3. **Synonym Extraction & Filtering**  \n",
    "   - Calls `get_wordnet_synonyms` to retrieve up to `max_synonyms` per selected term  \n",
    "   - Filters out any synonyms already present in the original query  \n",
    "\n",
    "4. **Query Assembly**  \n",
    "   - Appends the chosen synonyms to the original query text  \n",
    "   - Returns the expanded query string (Elasticsearch will re-analyze it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def is_expandable(pos):\n",
    "    return pos.startswith('NN') or pos.startswith('JJ')  # nouns & adjectives\n",
    "    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_query_with_wordnet(query_text, tfidf_vectorizer, max_synonyms=1, n_expand=1):\n",
    "    \n",
    "    # For expansion decisions, work with the original query text\n",
    "    tokens = word_tokenize(query_text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Set for fast lookup\n",
    "    original_words = set(tokens)\n",
    "    \n",
    "    # Candidate words = noun/adjective, not stopword, alphabetic\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "\n",
    "    # Get IDF scores from vectorizer to identify important terms\n",
    "    # We need to preprocess the word to match vectorizer's vocabulary\n",
    "    idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    # Score each candidate by IDF from the corpus\n",
    "    scored = [\n",
    "        (word, idf_scores.get(es_like_preprocess(word), 0.0))  # preprocess just for vocabulary lookup\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    # Sort by IDF weight descending and keep top-n\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "    \n",
    "    # Expand only top words\n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        synonyms = get_wordnet_synonyms(word, max_synonyms) # Get more, filter later\n",
    "        for syn in synonyms:\n",
    "            if syn in original_words:\n",
    "                continue  # Skip if already in query\n",
    "            expanded_terms.append(syn)\n",
    "            if len(expanded_terms) >= max_synonyms:\n",
    "                break  # limit per word\n",
    "\n",
    "        # Return original query + expansion terms (let Elasticsearch handle preprocessing)\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the original IR2025 query set from the JSONL file into a list for subsequent expansion and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 queries.\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open('../data/trec-covid/queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "    print(f\"Loaded {len(queries)} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `expand_query_with_wordnet` function to each original query, adding an `\"expanded_text\"` field to store the expanded query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:35<00:00,  1.41query/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "print(\"Expanding Queries..\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_wordnet(query[\"text\"], vectorizer)\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the list of expanded queries (including `\"query_id\"`, original `\"text\"`, and `\"expanded_text\"`) to a new JSONL file for later retrieval experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_wordnet.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write(q)\n",
    "    print(\"✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Query Expansion with hypernet replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetches synonyms of hypernyms (i.e., \"parent\" concepts) for a given word using WordNet, up to a specified tree depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_hypernym_synonyms(word, max_synonyms=1, depth=1):\n",
    "    \"\"\"\n",
    "    Get synonyms of hypernyms for a given word.\n",
    "    \n",
    "    Args:\n",
    "        word: Target word.\n",
    "        max_synonyms: Max number of terms to return.\n",
    "        depth: How deep to go in the hypernym tree (1 = immediate parents).\n",
    "        \n",
    "    Returns:\n",
    "        A ranked list of synonym candidates.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    synonyms = set()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        # Explore hypernyms up to the given depth\n",
    "        current_level = [syn]\n",
    "        for _ in range(depth):\n",
    "            next_level = []\n",
    "            for s in current_level:\n",
    "                next_level.extend(s.hypernyms())\n",
    "            current_level = next_level\n",
    "\n",
    "        # Collect lemma names from hypernyms\n",
    "        for hyper in current_level:\n",
    "            for lemma in hyper.lemmas():\n",
    "                name = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if name != word and name.isalpha() and len(name.split()) == 1:\n",
    "                    synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency\n",
    "    ranked = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "    return ranked[:max_synonyms]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expands a search query by adding hypernyms (parent concepts) of the most \"important\" words in the query, ranked using TF-IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_hypernyms(query_text, tfidf_vectorizer, max_synonyms=1, n_expand=1, depth=1):\n",
    "    tokens = word_tokenize(query_text.lower())\n",
    "    tagged = pos_tag(tokens)\n",
    "    original_words = set(tokens)\n",
    "\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "\n",
    "    idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    scored = [\n",
    "        (word, idf_scores.get(es_like_preprocess(word), 0.0))\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "    \n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        hypernyms = get_wordnet_hypernym_synonyms(word, max_synonyms=max_synonyms, depth=depth)\n",
    "        for hyp in hypernyms:\n",
    "            if hyp in original_words:\n",
    "                continue\n",
    "            expanded_terms.append(hyp)\n",
    "            if len(expanded_terms) >= max_synonyms:\n",
    "                break\n",
    "\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script iterates over a list of queries, expands each query using hypernyms (with `expand_query_with_hypernyms`), and writes the expanded queries to a `.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries with Hypernyms Only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:32<00:00,  1.52query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hypernym-expanded queries saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_queries_hyper = []\n",
    "print(\"Expanding Queries with Hypernyms Only...\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_hypernyms(query[\"text\"], vectorizer)\n",
    "    expanded_queries_hyper.append(new_query)\n",
    "\n",
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_hypernyms.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries_hyper:\n",
    "        writer.write(q)\n",
    "print(\"✅ Hypernym-expanded queries saved to ../data/trec-covid/queries_expanded_hypernyms.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Retrieval with Expanded Queries\n",
    "\n",
    "Load the expanded queries from disk, then for each cutoff \\(k in \\{20, 30, 50\\}\\) perform a `match` search on the `text` field using the `\"expanded_text\"`. Collect the top-\\(k\\) document IDs and their scores into separate JSON run files under `results/phase_2/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 20:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 20: 100%|██████████| 50/50 [00:08<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_20_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 30: 100%|██████████| 50/50 [00:01<00:00, 39.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_30_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 50: 100%|██████████| 50/50 [00:01<00:00, 34.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_50_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_queries_phase_2(expanded_queries_path, label='WordNet'):\n",
    "    # Load queries\n",
    "    with open(expanded_queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Expanded Queries with {label} for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"expanded_text\"] # This is the key the expanded query is saved under\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": {\"text\": query_text}},\n",
    "                size=k\n",
    "            )\n",
    "            # print(response)\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_2/retrieval_top_{k}_{label.lower()}.json\")\n",
    "\n",
    "    return runs\n",
    "    \n",
    "runs_wordnet = process_queries_phase_2(\"../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 20:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 20: 100%|██████████| 50/50 [00:00<00:00, 80.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_20_hypernyms.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 30: 100%|██████████| 50/50 [00:00<00:00, 88.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_30_hypernyms.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 50: 100%|██████████| 50/50 [00:00<00:00, 81.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_50_hypernyms.json\n"
     ]
    }
   ],
   "source": [
    "runs_hypernyms = process_queries_phase_2(\"../data/trec-covid/queries_expanded_hypernyms.jsonl\", label='Hypernyms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the TREC‐COVID qrels file into a nested dictionary of the form `{query_id: {doc_id: relevance_score}}`. Also compute and print the average number of relevant documents per query for insight into dataset sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation of Expanded Runs with `pytrec_eval`\n",
    "\n",
    "Use the `compute_metrics` function to evaluate each expanded-query run (`k` = 20, 30, 50) by:\n",
    "\n",
    "- Computing **MAP** and **Precision@k** (for k = 5, 10, 15, 20) via `pytrec_eval.RelevanceEvaluator`  \n",
    "- Saving **per-query** metrics to `results/phase_2/per_query_metrics_top_<k>.json`  \n",
    "- Saving **average** metrics to `results/phase_2/average_metrics_top_<k>.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50_wordnet.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20'], label=\"WordNet\"):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "        #     num_docs = len(docs)\n",
    "        #     print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}_{label.lower()}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}_{label.lower()}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")\n",
    "        \n",
    "compute_metrics(qrels, runs_wordnet, 'phase_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20_hypernyms.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30_hypernyms.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50_hypernyms.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(qrels, runs_hypernyms, 'phase_2', label=\"Hypernyms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare Phase 1 & Phase 2 Metrics\n",
    "\n",
    "Define a helper `compare_phases` to load average metrics for each phase (e.g. Phase 1 vs. Phase 2) at cutoffs k ∈ {20, 30, 50} and display a consolidated DataFrame:\n",
    "\n",
    "- **Parameters**  \n",
    "  - `phases`: dict mapping phase names to file path patterns, e.g.  \n",
    "    ```python\n",
    "    {\n",
    "      \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "      \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\"\n",
    "    }\n",
    "    ```  \n",
    "  - `k_values`: list of cutoff values to compare (default `[20, 30, 50]`)  \n",
    "  - `metrics`: list of metric keys to include (default `['map','P_5','P_10','P_15','P_20']`)  \n",
    "\n",
    "- **Behavior**  \n",
    "  1. For each k, loads the corresponding JSON file for each phase.  \n",
    "  2. Extracts MAP and Precision@k metrics.  \n",
    "  3. Builds a pandas DataFrame indexed by k.  \n",
    "  4. Displays the comparison table side-by-side.\n",
    "\n",
    "- **Returns**  \n",
    "  - A `pandas.DataFrame` with rows for each k and columns for each phase’s MAP and avgPre@k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare ([20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 1 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "3f591ee4-8f2e-4a2b-8536-6b11da436260",
       "rows": [
        [
         "20",
         "0.020569367599275835",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5479999999999999",
         "0.020553690675576487",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.020773378459637674",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999"
        ],
        [
         "30",
         "0.027752701467553504",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.028372537822998433",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.02860120113092701",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999"
        ],
        [
         "50",
         "0.039910502219297476",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.040848093544869804",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.040098644381552744",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 - WordNet MAP</th>\n",
       "      <th>Phase 2 - WordNet avgPre@5</th>\n",
       "      <th>Phase 2 - WordNet avgPre@10</th>\n",
       "      <th>Phase 2 - WordNet avgPre@15</th>\n",
       "      <th>Phase 2 - WordNet avgPre@20</th>\n",
       "      <th>Phase 2 - Hypernyms MAP</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@5</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@10</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@15</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.020773</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.028373</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.040848</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.040099</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.020569              0.64              0.582              0.564   \n",
       "30     0.027753              0.64              0.582              0.564   \n",
       "50     0.039911              0.64              0.582              0.564   \n",
       "\n",
       "    Phase 1 avgPre@20  Phase 2 - WordNet MAP  Phase 2 - WordNet avgPre@5  \\\n",
       "k                                                                          \n",
       "20              0.548               0.020554                       0.608   \n",
       "30              0.549               0.028373                       0.608   \n",
       "50              0.549               0.040848                       0.608   \n",
       "\n",
       "    Phase 2 - WordNet avgPre@10  Phase 2 - WordNet avgPre@15  \\\n",
       "k                                                              \n",
       "20                        0.586                        0.556   \n",
       "30                        0.586                        0.556   \n",
       "50                        0.586                        0.556   \n",
       "\n",
       "    Phase 2 - WordNet avgPre@20  Phase 2 - Hypernyms MAP  \\\n",
       "k                                                          \n",
       "20                        0.538                 0.020773   \n",
       "30                        0.538                 0.028601   \n",
       "50                        0.538                 0.040099   \n",
       "\n",
       "    Phase 2 - Hypernyms avgPre@5  Phase 2 - Hypernyms avgPre@10  \\\n",
       "k                                                                 \n",
       "20                         0.636                          0.574   \n",
       "30                         0.636                          0.574   \n",
       "50                         0.636                          0.574   \n",
       "\n",
       "    Phase 2 - Hypernyms avgPre@15  Phase 2 - Hypernyms avgPre@20  \n",
       "k                                                                 \n",
       "20                       0.545333                          0.537  \n",
       "30                       0.545333                          0.537  \n",
       "50                       0.545333                          0.537  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2 - WordNet\": \"../results/phase_2/average_metrics_top_{}_wordnet.json\",\n",
    "    \"Phase 2 - Hypernyms\": \"../results/phase_2/average_metrics_top_{}_hypernyms.json\"\n",
    "}\n",
    "_ = compare_phases(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **k** | **Phase 1 MAP** | **Phase 1 avgPre\\@5** | **Phase 1 avgPre\\@10** | **Phase 1 avgPre\\@15** | **Phase 1 avgPre\\@20** |\n",
    "| :---: | --------------: | --------------------: | ---------------------: | ---------------------: | ---------------------: |\n",
    "|   20  |        0.020569 |                 0.640 |                  0.582 |                  0.564 |                  0.548 |\n",
    "|   30  |        0.027753 |                 0.640 |                  0.582 |                  0.564 |                  0.549 |\n",
    "|   50  |        0.039911 |                 0.640 |                  0.582 |                  0.564 |                  0.549 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **k** | **WordNet MAP** | **WordNet avgPre\\@5** | **WordNet avgPre\\@10** | **WordNet avgPre\\@15** | **WordNet avgPre\\@20** |\n",
    "| :---: | --------------: | --------------------: | ---------------------: | ---------------------: | ---------------------: |\n",
    "|   20  |        0.020554 |                 0.608 |                  0.586 |                  0.556 |                  0.538 |\n",
    "|   30  |        0.028373 |                 0.608 |                  0.586 |                  0.556 |                  0.538 |\n",
    "|   50  |        0.040848 |                 0.608 |                  0.586 |                  0.556 |                  0.538 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **k** | **Hypernyms MAP** | **Hypernyms avgPre\\@5** | **Hypernyms avgPre\\@10** | **Hypernyms avgPre\\@15** | **Hypernyms avgPre\\@20** |\n",
    "| :---: | ----------------: | ----------------------: | -----------------------: | -----------------------: | -----------------------: |\n",
    "|   20  |          0.020773 |                   0.636 |                    0.574 |                 0.545333 |                    0.537 |\n",
    "|   30  |          0.028601 |                   0.636 |                    0.574 |                 0.545333 |                    0.537 |\n",
    "|   50  |          0.040099 |                   0.636 |                    0.574 |                 0.545333 |                    0.537 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **k** | **Phase 1** | **Phase 2 A (WordNet)** |             **Δ A** | **Phase 2 B (Hypernyms)** |             **Δ B** |\n",
    "| :---: | ----------: | ----------------------: | ------------------: | ------------------------: | ------------------: |\n",
    "|   20  |    0.020569 |                0.020554 | −0.000015 (−0.07 %) |                  0.020773 | +0.000204 (+0.99 %) |\n",
    "|   30  |    0.027753 |                0.028373 |  +0.000620 (+2.2 %) |                  0.028601 | +0.000848 (+3.06 %) |\n",
    "|   50  |    0.039911 |                0.040848 | +0.000937 (+2.35 %) |                  0.040099 | +0.000188 (+0.47 %) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|           | **Phase 1** | **Phase 2 A** |          **Δ A** | **Phase 2 B** |          **Δ B** |\n",
    "| :-------: | ----------: | ------------: | ---------------: | ------------: | ---------------: |\n",
    "| avgPre\\@5 |       0.640 |         0.608 | −0.032 (−5.0 pp) |         0.636 | −0.004 (−0.6 pp) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query Expansion **BEFORE** Query Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('../data/trec-covid/corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "def build_and_save_tfidf_model(corpus, output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Build TF-IDF model from corpus, compute statistics, and save the model.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of documents with 'text' field\n",
    "        output_dir: Directory to save models and statistics\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics_dict)\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize counters for statistics\n",
    "    all_tokens = []\n",
    "    total_tokens = 0\n",
    "    unique_tokens = set()\n",
    "    statistics = {}\n",
    "\n",
    "    # Preprocess with detailed statistics\n",
    "    print(\"Preprocessing corpus...\")\n",
    "    preprocessed_corpus = []\n",
    "    for doc in tqdm(corpus, desc=\"Preprocessing documents\", unit=\"doc\"):\n",
    "        text = doc[\"text\"]\n",
    "        tokens = text.split()\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "        # Update statistics\n",
    "        total_tokens += len(tokens)\n",
    "        unique_tokens.update(tokens)\n",
    "        \n",
    "        preprocessed_corpus.append(text)\n",
    "    \n",
    "    \n",
    "    word_counts = Counter(all_tokens)\n",
    "    global top_50_words\n",
    "    top_50_words = [w for w, _ in word_counts.most_common(50)]\n",
    "    print(\"Top 50 words:\", top_50_words)\n",
    "\n",
    "    # Save preprocessing statistics\n",
    "    statistics['preprocessing'] = {\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': len(unique_tokens),\n",
    "        'average_tokens_per_doc': total_tokens/len(corpus)\n",
    "    }\n",
    "\n",
    "    print(f\"\\nPreprocessing statistics:\")\n",
    "    print(f\"- Total tokens: {total_tokens:,}\")\n",
    "    print(f\"- Unique tokens: {len(unique_tokens):,}\")\n",
    "    print(f\"- Average tokens per document: {total_tokens/len(corpus):,.1f}\")\n",
    "\n",
    "    # Build TF-IDF model with detailed progress\n",
    "    print(\"\\nBuilding TF-IDF model...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=False, stop_words=None)\n",
    "\n",
    "    with tqdm(total=3, desc=\"TF-IDF computation\") as pbar:\n",
    "        # Fit the vectorizer\n",
    "        tfidf_vectorizer.fit(preprocessed_corpus)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # Calculate IDF scores\n",
    "        idf_scores = dict(zip(feature_names, tfidf_vectorizer.idf_))\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Calculate and save TF-IDF statistics\n",
    "    idf_values = list(idf_scores.values())\n",
    "    statistics['tfidf'] = {\n",
    "        'vocabulary_size': len(idf_scores),\n",
    "        'average_idf': float(np.mean(idf_values)),\n",
    "        'max_idf': float(max(idf_values)),\n",
    "        'min_idf': float(min(idf_values))\n",
    "    }\n",
    "\n",
    "    print(\"\\nTF-IDF statistics:\")\n",
    "    print(f\"- Vocabulary size: {len(idf_scores):,} terms\")\n",
    "    print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "    print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "    print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    "\n",
    "    # Save everything\n",
    "    print(\"\\nSaving models and statistics...\")\n",
    "    try:\n",
    "        # Save vectorizer\n",
    "        joblib.dump(tfidf_vectorizer, os.path.join(output_dir, 'tfidf_vectorizer_other.joblib'))\n",
    "        \n",
    "        # Save IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores_other.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(idf_scores, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        # Save statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics_other.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(statistics, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "        print(\"\\n✅ Saved successfully:\")\n",
    "        print(f\"- Vectorizer: {os.path.join(output_dir, 'tfidf_vectorizer_other.joblib')}\")\n",
    "        print(f\"- IDF scores: {os.path.join(output_dir, 'idf_scores_other.json')}\")\n",
    "        print(f\"- Statistics: {os.path.join(output_dir, 'tfidf_statistics_other.json')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error saving files: {e}\")\n",
    "        \n",
    "    return tfidf_vectorizer, idf_scores\n",
    "\n",
    "# Function to load the saved model\n",
    "def load_tfidf_model(output_dir=\"../models\"):\n",
    "    \"\"\"\n",
    "    Load the saved TF-IDF model, IDF scores, and statistics.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory where models and statistics are saved\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (vectorizer, idf_scores, statistics)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load vectorizer\n",
    "        vectorizer = joblib.load(os.path.join(output_dir, 'tfidf_vectorizer_other.joblib'))\n",
    "        \n",
    "        # Load IDF scores\n",
    "        with open(os.path.join(output_dir, 'idf_scores_other.json'), 'r', encoding='utf-8') as f:\n",
    "            idf_scores = json.load(f)\n",
    "            \n",
    "        # Load statistics\n",
    "        with open(os.path.join(output_dir, 'tfidf_statistics_other.json'), 'r', encoding='utf-8') as f:\n",
    "            statistics = json.load(f)\n",
    "        \n",
    "        print(\"\\nModel validation:\")\n",
    "        print(f\"- Vocabulary size: {len(idf_scores):,}\")\n",
    "        print(f\"- Average IDF: {statistics['tfidf']['average_idf']:.2f}\")\n",
    "        print(f\"- Max IDF: {statistics['tfidf']['max_idf']:.2f}\")\n",
    "        print(f\"- Min IDF: {statistics['tfidf']['min_idf']:.2f}\")\n",
    " \n",
    "        print(\"✅ Model loaded successfully\")\n",
    "        return vectorizer, idf_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        return None, None\n",
    "        \n",
    "# Transform new text using the loaded vectorizer\n",
    "def transform_text(text, vectorizer):\n",
    "    \"\"\"Transform new text using the loaded vectorizer\"\"\"\n",
    "    try:\n",
    "        transformed = vectorizer.transform([text])\n",
    "        return transformed\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error transforming text: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model validation:\n",
      "- Vocabulary size: 241,243\n",
      "- Average IDF: 11.57\n",
      "- Max IDF: 12.36\n",
      "- Min IDF: 1.31\n",
      "✅ Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Test loading\n",
    "vectorizer, idf_scores = load_tfidf_model()\n",
    "if not vectorizer and not idf_scores:\n",
    "    # Build and save the model\n",
    "    vectorizer, idf_scores = build_and_save_tfidf_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_wordnet_synonyms(word, max_synonyms=3):\n",
    "    synonyms = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace(\"_\", \" \").lower()\n",
    "\n",
    "            # Filter out:\n",
    "            if name == word: # Skip the original word\n",
    "                continue\n",
    "            if len(name.split()) > 1:  # Skip multi-word phrases\n",
    "                continue\n",
    "            if not name.isalpha(): # Skip non-alphabetic words\n",
    "                continue\n",
    "\n",
    "            synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency (most-used synonyms first)\n",
    "    ranked_synonyms = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "\n",
    "    return ranked_synonyms[:max_synonyms] # Return up to max_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def is_expandable(pos):\n",
    "    return pos.startswith('NN') or pos.startswith('JJ')  # nouns & adjectives\n",
    "    \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_query_with_wordnet(query_text, tfidf_vectorizer, max_synonyms=1, n_expand=1):\n",
    "    \n",
    "    # For expansion decisions, work with the original query text\n",
    "    tokens = word_tokenize(query_text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Set for fast lookup\n",
    "    original_words = set(tokens)\n",
    "    \n",
    "    # Candidate words = noun/adjective, not stopword, alphabetic\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "\n",
    "    # Get IDF scores from vectorizer to identify important terms\n",
    "    # We need to preprocess the word to match vectorizer's vocabulary\n",
    "    idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    # Score each candidate by IDF from the corpus\n",
    "    scored = [\n",
    "        (word, idf_scores.get(word, 0.0))\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    # Sort by IDF weight descending and keep top-n\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "    \n",
    "    # Expand only top words\n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        synonyms = get_wordnet_synonyms(word, max_synonyms) # Get more, filter later\n",
    "        for syn in synonyms:\n",
    "            if syn in original_words:\n",
    "                continue  # Skip if already in query\n",
    "            expanded_terms.append(syn)\n",
    "            if len(expanded_terms) >= max_synonyms:\n",
    "                break  # limit per word\n",
    "\n",
    "        # Return original query + expansion terms (let Elasticsearch handle preprocessing)\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 queries.\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open('../data/trec-covid/queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "    print(f\"Loaded {len(queries)} queries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.94query/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "print(\"Expanding Queries..\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_wordnet(query[\"text\"], vectorizer)\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet_other.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_wordnet_other.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write(q)\n",
    "    print(\"✅ Expanded queries saved to ../data/trec-covid/queries_expanded_wordnet_other.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_hypernym_synonyms(word, max_synonyms=1, depth=1):\n",
    "    \"\"\"\n",
    "    Get synonyms of hypernyms for a given word.\n",
    "    \n",
    "    Args:\n",
    "        word: Target word.\n",
    "        max_synonyms: Max number of terms to return.\n",
    "        depth: How deep to go in the hypernym tree (1 = immediate parents).\n",
    "        \n",
    "    Returns:\n",
    "        A ranked list of synonym candidates.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    synonyms = set()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        # Explore hypernyms up to the given depth\n",
    "        current_level = [syn]\n",
    "        for _ in range(depth):\n",
    "            next_level = []\n",
    "            for s in current_level:\n",
    "                next_level.extend(s.hypernyms())\n",
    "            current_level = next_level\n",
    "\n",
    "        # Collect lemma names from hypernyms\n",
    "        for hyper in current_level:\n",
    "            for lemma in hyper.lemmas():\n",
    "                name = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if name != word and name.isalpha() and len(name.split()) == 1:\n",
    "                    synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency\n",
    "    ranked = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "    return ranked[:max_synonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query_with_hypernyms(query_text, tfidf_vectorizer, max_synonyms=1, n_expand=1, depth=1):\n",
    "    tokens = word_tokenize(query_text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    original_words = set(tokens)\n",
    "\n",
    "    candidates = [\n",
    "        (word, pos) for word, pos in tagged\n",
    "        if word.isalpha() and word not in stop_words and is_expandable(pos)\n",
    "    ]\n",
    "\n",
    "    idf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    \n",
    "    scored = [\n",
    "        (word, idf_scores.get(word, 0.0))\n",
    "        for word, _ in candidates\n",
    "    ]\n",
    "\n",
    "    top_words = [word for word, _ in sorted(scored, key=lambda x: -x[1])[:n_expand]]\n",
    "    \n",
    "    expanded_terms = []\n",
    "    for word in top_words:\n",
    "        hypernyms = get_wordnet_hypernym_synonyms(word, max_synonyms=max_synonyms, depth=depth)\n",
    "        for hyp in hypernyms:\n",
    "            if hyp in original_words:\n",
    "                continue\n",
    "            expanded_terms.append(hyp)\n",
    "            if len(expanded_terms) >= max_synonyms:\n",
    "                break\n",
    "\n",
    "    return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding Queries with Hypernyms Only...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:23<00:00,  2.09query/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hypernym-expanded queries saved to ../data/trec-covid/queries_expanded_hypernyms_other.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_queries_hyper = []\n",
    "print(\"Expanding Queries with Hypernyms Only...\")\n",
    "for query in tqdm(queries, unit=\"query\"):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_hypernyms(query[\"text\"], vectorizer)\n",
    "    expanded_queries_hyper.append(new_query)\n",
    "\n",
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_hypernyms_other.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries_hyper:\n",
    "        writer.write(q)\n",
    "print(\"✅ Hypernym-expanded queries saved to ../data/trec-covid/queries_expanded_hypernyms_other.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "def load_qrels(qrels_path=\"../data/trec-covid/qrels/test.tsv\"):\n",
    "    qrels = {}\n",
    "    with open(qrels_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            qid = row['query-id']\n",
    "            docid = row['corpus-id']\n",
    "            relevance = int(row['score'])\n",
    "            qrels.setdefault(qid, {})[docid] = relevance\n",
    "\n",
    "    relevant_counts = Counter()\n",
    "    for qid, docs in qrels.items():\n",
    "        relevant_counts[qid] = sum(1 for rel in docs.values() if rel > 0)\n",
    "    print(\"Average number of relevant documents per query:\", int(sum(relevant_counts.values()) / len(relevant_counts)))\n",
    "\n",
    "    return qrels\n",
    "\n",
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries_phase_22(expanded_queries_path, label='WordNet'):\n",
    "    # Load queries\n",
    "    with open(expanded_queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_22\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Expanded Queries with {label} for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            query_text = query[\"expanded_text\"] # This is the key the expanded query is saved under\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\"match\": {\"text\": query_text}},\n",
    "                size=k\n",
    "            )\n",
    "            # print(response)\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_22/retrieval_top_{k}_{label.lower()}.json\")\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 20: 100%|██████████| 50/50 [00:03<00:00, 13.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_20_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 30: 100%|██████████| 50/50 [00:02<00:00, 20.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_30_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with WordNet for run with k = 50: 100%|██████████| 50/50 [00:01<00:00, 28.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_50_wordnet.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "runs_wordnet = process_queries_phase_22(\"../data/trec-covid/queries_expanded_wordnet_other.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50_wordnet.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics(qrels, runs, folder, metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20'], label=\"WordNet\"):    \n",
    "    # Metrics to Evaluate\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {'map', 'P'})\n",
    "    \n",
    "    for run_name, run in runs.items():\n",
    "        k = run_name.split(\"_\")[1]\n",
    "        print(f\"Computing metrics for run with k = {k}\")\n",
    "        \n",
    "        # Verify how many documents were retrieved per query\n",
    "        # for query_id, docs in run.items():\n",
    "        #     num_docs = len(docs)\n",
    "        #     print(f\"Query ID: {query_id} - Retrieved Documents: {num_docs}\")\n",
    "            \n",
    "        results = evaluator.evaluate(run)\n",
    "        \n",
    "        #Print available metrics for debugging\n",
    "        # first_query = list(results.keys())[0]\n",
    "        # print(f\"Available metrics for {first_query}: {list(results[first_query].keys())}\")\n",
    "        \n",
    "        # Compute average metrics\n",
    "        avg_scores = {metric: 0.0 for metric in metrics}\n",
    "        num_queries = len(results)\n",
    "        \n",
    "        for res in results.values():\n",
    "            for metric in metrics:\n",
    "                avg_scores[metric] += res.get(metric, 0.0)\n",
    "        \n",
    "        for metric in metrics:\n",
    "            avg_scores[metric] /= num_queries\n",
    "                                                                                                                                               \n",
    "        # Prepare output directory\n",
    "        output_dir = os.path.join(\"../results\", folder)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save per-query metrics\n",
    "        per_query_path = os.path.join(output_dir, f\"per_query_metrics_top_{k}_{label.lower()}.json\")\n",
    "        with open(per_query_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        \n",
    "        # Save average metrics\n",
    "        avg_metrics_path = os.path.join(output_dir, f\"average_metrics_top_{k}_{label.lower()}.json\")\n",
    "        with open(avg_metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(avg_scores, f, indent=4)\n",
    "        \n",
    "        print(f\"✅ Per-query metrics saved to: {per_query_path}\")\n",
    "        print(f\"✅ Average metrics saved to: {avg_metrics_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_20_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_20_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_30_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_30_wordnet.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_50_wordnet.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_50_wordnet.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(qrels, runs_wordnet, 'phase_22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 20: 100%|██████████| 50/50 [00:00<00:00, 52.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_20_hypernyms.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 30: 100%|██████████| 50/50 [00:00<00:00, 55.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_30_hypernyms.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Expanded Queries with Hypernyms for run with k = 50: 100%|██████████| 50/50 [00:01<00:00, 46.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_22/retrieval_top_50_hypernyms.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "runs_hypernyms = process_queries_phase_22(\"../data/trec-covid/queries_expanded_hypernyms_other.jsonl\", label='Hypernyms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_20_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_20_hypernyms.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_30_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_30_hypernyms.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_22\\per_query_metrics_top_50_hypernyms.json\n",
      "✅ Average metrics saved to: ../results\\phase_22\\average_metrics_top_50_hypernyms.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(qrels, runs_hypernyms, 'phase_22', label=\"Hypernyms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 2 - WordNet MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - WordNet avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "f83b99be-99f5-43c4-902b-c859aa27151d",
       "rows": [
        [
         "20",
         "0.020553690675576487",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.020366052204888678",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999"
        ],
        [
         "30",
         "0.028372537822998433",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.02852086390809521",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999"
        ],
        [
         "50",
         "0.040848093544869804",
         "0.608",
         "0.586",
         "0.556",
         "0.5379999999999999",
         "0.04066395728080773",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 2 - WordNet MAP</th>\n",
       "      <th>Phase 2 - WordNet avgPre@5</th>\n",
       "      <th>Phase 2 - WordNet avgPre@10</th>\n",
       "      <th>Phase 2 - WordNet avgPre@15</th>\n",
       "      <th>Phase 2 - WordNet avgPre@20</th>\n",
       "      <th>Phase 22 - WordNet MAP</th>\n",
       "      <th>Phase 22 - WordNet avgPre@5</th>\n",
       "      <th>Phase 22 - WordNet avgPre@10</th>\n",
       "      <th>Phase 22 - WordNet avgPre@15</th>\n",
       "      <th>Phase 22 - WordNet avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020554</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.020366</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.028373</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.028521</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.040848</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 2 - WordNet MAP  Phase 2 - WordNet avgPre@5  \\\n",
       "k                                                       \n",
       "20               0.020554                       0.608   \n",
       "30               0.028373                       0.608   \n",
       "50               0.040848                       0.608   \n",
       "\n",
       "    Phase 2 - WordNet avgPre@10  Phase 2 - WordNet avgPre@15  \\\n",
       "k                                                              \n",
       "20                        0.586                        0.556   \n",
       "30                        0.586                        0.556   \n",
       "50                        0.586                        0.556   \n",
       "\n",
       "    Phase 2 - WordNet avgPre@20  Phase 22 - WordNet MAP  \\\n",
       "k                                                         \n",
       "20                        0.538                0.020366   \n",
       "30                        0.538                0.028521   \n",
       "50                        0.538                0.040664   \n",
       "\n",
       "    Phase 22 - WordNet avgPre@5  Phase 22 - WordNet avgPre@10  \\\n",
       "k                                                               \n",
       "20                        0.596                         0.588   \n",
       "30                        0.596                         0.588   \n",
       "50                        0.596                         0.588   \n",
       "\n",
       "    Phase 22 - WordNet avgPre@15  Phase 22 - WordNet avgPre@20  \n",
       "k                                                               \n",
       "20                         0.552                         0.538  \n",
       "30                         0.552                         0.538  \n",
       "50                         0.552                         0.538  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 2 - WordNet\": \"../results/phase_2/average_metrics_top_{}_wordnet.json\",\n",
    "    \"Phase 22 - WordNet\": \"../results/phase_22/average_metrics_top_{}_wordnet.json\",\n",
    "}\n",
    "compare_phases(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Phase 2 - WordNet MAP</th>\n",
    "      <th>Phase 2 - WordNet avgPre@5</th>\n",
    "      <th>Phase 2 - WordNet avgPre@10</th>\n",
    "      <th>Phase 2 - WordNet avgPre@15</th>\n",
    "      <th>Phase 2 - WordNet avgPre@20</th>\n",
    "      <th>Phase 22 - WordNet MAP</th>\n",
    "      <th>Phase 22 - WordNet avgPre@5</th>\n",
    "      <th>Phase 22 - WordNet avgPre@10</th>\n",
    "      <th>Phase 22 - WordNet avgPre@15</th>\n",
    "      <th>Phase 22 - WordNet avgPre@20</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>k</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>20</th>\n",
    "      <td>0.020554</td>\n",
    "      <td>0.608</td>\n",
    "      <td>0.586</td>\n",
    "      <td>0.556</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.020366</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>30</th>\n",
    "      <td>0.028373</td>\n",
    "      <td>0.608</td>\n",
    "      <td>0.586</td>\n",
    "      <td>0.556</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.028521</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>50</th>\n",
    "      <td>0.040848</td>\n",
    "      <td>0.608</td>\n",
    "      <td>0.586</td>\n",
    "      <td>0.556</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.040664</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **k** | **MAP**<br>(Preprocess → Expand) | **MAP**<br>(Expand → Preprocess) | **Δ MAP** | **avgPre@5**       | **avgPre@10**      | **avgPre@15**      | **avgPre@20**      |\n",
    "|----------------|----------------------------------|----------------------------------|-----------|---------------------|---------------------|---------------------|---------------------|\n",
    "| **20**         | 0.020554                         | 0.020366                         | −0.000188 | 0.608 → **0.596**   | 0.586 → **0.588**   | 0.556 → **0.552**   | 0.538 → **0.538**   |\n",
    "| **30**         | 0.028373                         | 0.028521                         | +0.000148 | 0.608 → **0.596**   | 0.586 → **0.588**   | 0.556 → **0.552**   | 0.538 → **0.538**   |\n",
    "| **50**         | 0.040848                         | 0.040664                         | −0.000184 | 0.608 → **0.596**   | 0.586 → **0.588**   | 0.556 → **0.552**   | 0.538 → **0.538**   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 2 - Hypernyms MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 2 - Hypernyms avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "de23ab12-e5f3-4e29-874a-50ae503eee68",
       "rows": [
        [
         "20",
         "0.020773378459637674",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999",
         "0.02076097122404217",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ],
        [
         "30",
         "0.02860120113092701",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999",
         "0.028541743542430557",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ],
        [
         "50",
         "0.040098644381552744",
         "0.6360000000000001",
         "0.574",
         "0.5453333333333333",
         "0.5369999999999999",
         "0.040005566853597664",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 2 - Hypernyms MAP</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@5</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@10</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@15</th>\n",
       "      <th>Phase 2 - Hypernyms avgPre@20</th>\n",
       "      <th>Phase 22 - Hypernyms MAP</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@5</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@10</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@15</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020773</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.028601</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.028542</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.040099</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.545333</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 2 - Hypernyms MAP  Phase 2 - Hypernyms avgPre@5  \\\n",
       "k                                                           \n",
       "20                 0.020773                         0.636   \n",
       "30                 0.028601                         0.636   \n",
       "50                 0.040099                         0.636   \n",
       "\n",
       "    Phase 2 - Hypernyms avgPre@10  Phase 2 - Hypernyms avgPre@15  \\\n",
       "k                                                                  \n",
       "20                          0.574                       0.545333   \n",
       "30                          0.574                       0.545333   \n",
       "50                          0.574                       0.545333   \n",
       "\n",
       "    Phase 2 - Hypernyms avgPre@20  Phase 22 - Hypernyms MAP  \\\n",
       "k                                                             \n",
       "20                          0.537                  0.020761   \n",
       "30                          0.537                  0.028542   \n",
       "50                          0.537                  0.040006   \n",
       "\n",
       "    Phase 22 - Hypernyms avgPre@5  Phase 22 - Hypernyms avgPre@10  \\\n",
       "k                                                                   \n",
       "20                           0.64                           0.576   \n",
       "30                           0.64                           0.576   \n",
       "50                           0.64                           0.576   \n",
       "\n",
       "    Phase 22 - Hypernyms avgPre@15  Phase 22 - Hypernyms avgPre@20  \n",
       "k                                                                   \n",
       "20                        0.546667                           0.534  \n",
       "30                        0.546667                           0.534  \n",
       "50                        0.546667                           0.534  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 2 - Hypernyms\": \"../results/phase_2/average_metrics_top_{}_hypernyms.json\",\n",
    "    \"Phase 22 - Hypernyms\": \"../results/phase_22/average_metrics_top_{}_hypernyms.json\"\n",
    "}\n",
    "compare_phases(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Phase 2 - Hypernyms MAP</th>\n",
    "      <th>Phase 2 - Hypernyms avgPre@5</th>\n",
    "      <th>Phase 2 - Hypernyms avgPre@10</th>\n",
    "      <th>Phase 2 - Hypernyms avgPre@15</th>\n",
    "      <th>Phase 2 - Hypernyms avgPre@20</th>\n",
    "      <th>Phase 22 - Hypernyms MAP</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@5</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@10</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@15</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@20</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>k</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>20</th>\n",
    "      <td>0.020773</td>\n",
    "      <td>0.636</td>\n",
    "      <td>0.574</td>\n",
    "      <td>0.545333</td>\n",
    "      <td>0.537</td>\n",
    "      <td>0.020761</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>30</th>\n",
    "      <td>0.028601</td>\n",
    "      <td>0.636</td>\n",
    "      <td>0.574</td>\n",
    "      <td>0.545333</td>\n",
    "      <td>0.537</td>\n",
    "      <td>0.028542</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>50</th>\n",
    "      <td>0.040099</td>\n",
    "      <td>0.636</td>\n",
    "      <td>0.574</td>\n",
    "      <td>0.545333</td>\n",
    "      <td>0.537</td>\n",
    "      <td>0.040006</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Cutoff (k)** | **MAP**<br>(Preprocess → Expand) | **MAP**<br>(Expand → Preprocess) | **Δ MAP** | **avgPre@5**       | **avgPre@10**      | **avgPre@15**      | **avgPre@20**      |\n",
    "|----------------|----------------------------------|----------------------------------|-----------|---------------------|---------------------|---------------------|---------------------|\n",
    "| **20**         | 0.020773                         | 0.020761                         | −0.000012 | 0.636 → **0.640**   | 0.574 → **0.576**   | 0.545 → **0.547**   | 0.537 → **0.534**   |\n",
    "| **30**         | 0.028601                         | 0.028542                         | −0.000059 | 0.636 → **0.640**   | 0.574 → **0.576**   | 0.545 → **0.547**   | 0.537 → **0.534**   |\n",
    "| **50**         | 0.040099                         | 0.040006                         | −0.000093 | 0.636 → **0.640**   | 0.574 → **0.576**   | 0.545 → **0.547**   | 0.537 → **0.534**   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "k",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Phase 1 MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 1 avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - WordNet avgPre@20",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@10",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@15",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Phase 22 - Hypernyms avgPre@20",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d599e5de-e20d-4b1c-8d6c-b77c9c00c85c",
       "rows": [
        [
         "20",
         "0.020569367599275835",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5479999999999999",
         "0.020366052204888678",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999",
         "0.02076097122404217",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ],
        [
         "30",
         "0.027752701467553504",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.02852086390809521",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999",
         "0.028541743542430557",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ],
        [
         "50",
         "0.039910502219297476",
         "0.6400000000000001",
         "0.5819999999999999",
         "0.5640000000000001",
         "0.5489999999999999",
         "0.04066395728080773",
         "0.596",
         "0.588",
         "0.552",
         "0.5379999999999999",
         "0.040005566853597664",
         "0.6400000000000001",
         "0.5759999999999998",
         "0.5466666666666667",
         "0.5339999999999999"
        ]
       ],
       "shape": {
        "columns": 15,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 22 - WordNet MAP</th>\n",
       "      <th>Phase 22 - WordNet avgPre@5</th>\n",
       "      <th>Phase 22 - WordNet avgPre@10</th>\n",
       "      <th>Phase 22 - WordNet avgPre@15</th>\n",
       "      <th>Phase 22 - WordNet avgPre@20</th>\n",
       "      <th>Phase 22 - Hypernyms MAP</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@5</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@10</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@15</th>\n",
       "      <th>Phase 22 - Hypernyms avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.020569</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.020366</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.027753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.028521</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.028542</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.039911</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.040664</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.538</td>\n",
       "      <td>0.040006</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.020569              0.64              0.582              0.564   \n",
       "30     0.027753              0.64              0.582              0.564   \n",
       "50     0.039911              0.64              0.582              0.564   \n",
       "\n",
       "    Phase 1 avgPre@20  Phase 22 - WordNet MAP  Phase 22 - WordNet avgPre@5  \\\n",
       "k                                                                            \n",
       "20              0.548                0.020366                        0.596   \n",
       "30              0.549                0.028521                        0.596   \n",
       "50              0.549                0.040664                        0.596   \n",
       "\n",
       "    Phase 22 - WordNet avgPre@10  Phase 22 - WordNet avgPre@15  \\\n",
       "k                                                                \n",
       "20                         0.588                         0.552   \n",
       "30                         0.588                         0.552   \n",
       "50                         0.588                         0.552   \n",
       "\n",
       "    Phase 22 - WordNet avgPre@20  Phase 22 - Hypernyms MAP  \\\n",
       "k                                                            \n",
       "20                         0.538                  0.020761   \n",
       "30                         0.538                  0.028542   \n",
       "50                         0.538                  0.040006   \n",
       "\n",
       "    Phase 22 - Hypernyms avgPre@5  Phase 22 - Hypernyms avgPre@10  \\\n",
       "k                                                                   \n",
       "20                           0.64                           0.576   \n",
       "30                           0.64                           0.576   \n",
       "50                           0.64                           0.576   \n",
       "\n",
       "    Phase 22 - Hypernyms avgPre@15  Phase 22 - Hypernyms avgPre@20  \n",
       "k                                                                   \n",
       "20                        0.546667                           0.534  \n",
       "30                        0.546667                           0.534  \n",
       "50                        0.546667                           0.534  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 22 - WordNet\": \"../results/phase_22/average_metrics_top_{}_wordnet.json\",\n",
    "    \"Phase 22 - Hypernyms\": \"../results/phase_22/average_metrics_top_{}_hypernyms.json\"\n",
    "}\n",
    "compare_phases(phases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Phase 1 MAP</th>\n",
    "      <th>Phase 1 avgPre@5</th>\n",
    "      <th>Phase 1 avgPre@10</th>\n",
    "      <th>Phase 1 avgPre@15</th>\n",
    "      <th>Phase 1 avgPre@20</th>\n",
    "      <th>Phase 22 - WordNet MAP</th>\n",
    "      <th>Phase 22 - WordNet avgPre@5</th>\n",
    "      <th>Phase 22 - WordNet avgPre@10</th>\n",
    "      <th>Phase 22 - WordNet avgPre@15</th>\n",
    "      <th>Phase 22 - WordNet avgPre@20</th>\n",
    "      <th>Phase 22 - Hypernyms MAP</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@5</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@10</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@15</th>\n",
    "      <th>Phase 22 - Hypernyms avgPre@20</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>k</th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "      <th></th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>20</th>\n",
    "      <td>0.020569</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.582</td>\n",
    "      <td>0.564</td>\n",
    "      <td>0.548</td>\n",
    "      <td>0.020366</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.020761</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>30</th>\n",
    "      <td>0.027753</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.582</td>\n",
    "      <td>0.564</td>\n",
    "      <td>0.549</td>\n",
    "      <td>0.028521</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.028542</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>50</th>\n",
    "      <td>0.039911</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.582</td>\n",
    "      <td>0.564</td>\n",
    "      <td>0.549</td>\n",
    "      <td>0.040664</td>\n",
    "      <td>0.596</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.552</td>\n",
    "      <td>0.538</td>\n",
    "      <td>0.040006</td>\n",
    "      <td>0.64</td>\n",
    "      <td>0.576</td>\n",
    "      <td>0.546667</td>\n",
    "      <td>0.534</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
