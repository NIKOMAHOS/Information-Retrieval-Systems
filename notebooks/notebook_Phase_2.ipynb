{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Query Expansion with Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ElasticSearch manually before running the notebook:\n",
    "On Windows:\n",
    "- Make sure you have at least JDK 17\n",
    "- Open a terminal and execute this (or run it as a Windows service):\n",
    "```bash\n",
    "C:\\path\\to\\elasticsearch-8.17.2\\bin\\elasticsearch.bat\n",
    "```\n",
    "- No Greek characters should be present in the path.\n",
    "- Leave that terminal window open.\n",
    "\n",
    "- If no password was autogenerated execute this to get one:\n",
    "```bash\n",
    ".\\bin\\elasticsearch-reset-password.bat -u elastic\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r \"..\\\\requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jsonlines\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pytrec_eval\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file from the current directory\n",
    "load_dotenv(\"..\\\\secrets\\\\secrets.env\")\n",
    "\n",
    "# Access environment variables\n",
    "es_host = os.getenv(\"ES_HOST\")\n",
    "es_user = os.getenv(\"ES_USERNAME\")\n",
    "es_pass = os.getenv(\"ES_PASSWORD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to ElasticSearch\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch(es_host, basic_auth=(es_user, es_pass))\n",
    "\n",
    "if es.ping():\n",
    "    print(\"✅ Connected to ElasticSearch\")\n",
    "else:\n",
    "    print(\"❌ Connection failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mitsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab') # instead of punkt: NLTK > 3.8.2 !\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '../data/trec-covid/'\n",
    "\n",
    "with jsonlines.open(input_dir + 'corpus.jsonl') as reader:\n",
    "    corpus = [obj for obj in reader]\n",
    "\n",
    "with jsonlines.open(input_dir + 'queries.jsonl') as reader:\n",
    "    queries = [obj for obj in reader]\n",
    "\n",
    "test_df = pd.read_csv(input_dir + 'qrels/' + 'test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_wordnet_synonyms(word, max_synonyms=3):\n",
    "    synonyms = set()\n",
    "    word = word.lower()\n",
    "\n",
    "    for syn in wn.synsets(word):\n",
    "        # Keep only nouns and adjectives\n",
    "        if syn.pos() in ('n', 'a'):\n",
    "            for lemma in syn.lemmas():\n",
    "                name = lemma.name().replace(\"_\", \" \").lower()\n",
    "\n",
    "                # Filter out:\n",
    "                if name == word: # Skip the original word\n",
    "                    continue\n",
    "                if len(name.split()) > 1:  # Skip multi-word phrases\n",
    "                    continue\n",
    "                if not name.isalpha(): # Skip non-alphabetic words\n",
    "                    continue\n",
    "\n",
    "                synonyms.add(name)\n",
    "\n",
    "    # Rank by frequency (most-used synonyms first)\n",
    "    ranked_synonyms = sorted(synonyms, key=lambda s: -sum(lemma.count() for syn in wn.synsets(s) for lemma in syn.lemmas() if lemma.name().lower() == s))\n",
    "\n",
    "    return ranked_synonyms[:max_synonyms] # Return up to max_synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def expand_query_with_synonyms(query_text, max_synonyms=3):\n",
    "  expanded_terms = []\n",
    "  for word, pos in pos_tag(word_tokenize(query_text.lower())):\n",
    "    # Only expand nouns and adjectives, skip stopwords and non-alphabetic tokens\n",
    "      if word.isalpha() and word not in stop_words and pos.startswith(('NN', 'JJ')):\n",
    "          synonyms = get_wordnet_synonyms(word, max_synonyms)\n",
    "          expanded_terms.extend(synonyms)\n",
    "  return query_text + \" \" + \" \".join(expanded_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 284.24it/s]\n"
     ]
    }
   ],
   "source": [
    "expanded_queries = []\n",
    "for query in tqdm(queries):\n",
    "    new_query = query.copy()\n",
    "    new_query[\"expanded_text\"] = expand_query_with_synonyms(query[\"text\"])\n",
    "    expanded_queries.append(new_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to queries_expanded_wordnet.jsonl\n"
     ]
    }
   ],
   "source": [
    "with jsonlines.open(\"../data/trec-covid/queries_expanded_wordnet.jsonl\", mode='w') as writer:\n",
    "    for q in expanded_queries:\n",
    "        writer.write({\n",
    "            \"_id\": q[\"_id\"],\n",
    "            \"text\": q[\"expanded_text\"]\n",
    "        })\n",
    "    print(\"✅ Expanded queries saved to queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_queries_phase_2(queries_path):\n",
    "    # Load queries\n",
    "    with open(queries_path, 'r', encoding='utf-8') as f:\n",
    "        queries = [json.loads(line) for line in f]\n",
    "\n",
    "    INDEX_NAME = \"ir2025-index\"\n",
    "    k_values = [20, 30, 50]\n",
    "\n",
    "    runs = {f\"run_{k}\": {} for k in k_values}\n",
    "    for k in k_values:\n",
    "        output_dir = f\"../results/phase_2\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        for query in tqdm(queries, desc=f\"Processing Queries for run with k = {k}\"):\n",
    "            qid = query[\"_id\"]\n",
    "            original_text = query[\"text\"]\n",
    "            query_text = expand_query_with_synonyms(original_text)\n",
    "            \n",
    "            # This tells ElasticSearch:\n",
    "            # - “Give higher importance to the original query terms (boost)”\n",
    "            # - “But also consider the synonyms (with normal weight)”\n",
    "            # => This prevents the expanded terms from drowning out the original intent.\n",
    "            response = es.search(\n",
    "                index=INDEX_NAME,\n",
    "                query={\n",
    "                    \"bool\": {\n",
    "                        \"should\": [\n",
    "                            { \"match\": { \"text\": { \"query\": original_text, \"boost\": 3 } }},\n",
    "                            { \"match\": { \"text\": query_text }}\n",
    "                        ]\n",
    "                    }\n",
    "                },\n",
    "                size=k\n",
    "            )\n",
    "\n",
    "            runs[f\"run_{k}\"][qid] = {hit[\"_id\"]: hit[\"_score\"] for hit in response[\"hits\"][\"hits\"]}\n",
    "\n",
    "        # Save each run\n",
    "        with open(os.path.join(output_dir, f'retrieval_top_{k}.json'), 'w', encoding='utf-8') as f:\n",
    "            json.dump(runs[f\"run_{k}\"], f, ensure_ascii=False, indent=4)\n",
    "            print(f\"✅ Results saved to: ../results/phase_2/retrieval_top_{k}.json\")\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 20: 100%|██████████| 50/50 [00:01<00:00, 35.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_20.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 30: 100%|██████████| 50/50 [00:01<00:00, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_30.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Queries for run with k = 50: 100%|██████████| 50/50 [00:01<00:00, 32.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Results saved to: ../results/phase_2/retrieval_top_50.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "runs = process_queries_phase_2(\"../data/trec-covid/queries_expanded_wordnet.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of relevant documents per query: 493\n"
     ]
    }
   ],
   "source": [
    "qrels = load_qrels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics for run with k = 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_20.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_20.json\n",
      "\n",
      "Computing metrics for run with k = 30\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_30.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_30.json\n",
      "\n",
      "Computing metrics for run with k = 50\n",
      "✅ Per-query metrics saved to: ../results\\phase_2\\per_query_metrics_top_50.json\n",
      "✅ Average metrics saved to: ../results\\phase_2\\average_metrics_top_50.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(qrels, runs, 'phase_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the (AVG) results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_phases(phases, k_values=[20, 30, 50], metrics=['map', 'P_5', 'P_10', 'P_15', 'P_20']):\n",
    "    \"\"\"\n",
    "    Display and optionally compare retrieval metrics for 1 to 4 phases.\n",
    "    Parameters:\n",
    "    - phases: dict mapping phase names to base file paths, e.g.\n",
    "        {\n",
    "            \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "            \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "            ...\n",
    "        }\n",
    "    - k_values: list of cutoff values to compare ([20, 30, 50])\n",
    "    - metrics: list of TREC metric keys (['map', 'P_5', 'P_10'])\n",
    "\n",
    "    Returns:\n",
    "    - pandas DataFrame with metrics for all phases at each k\n",
    "    \"\"\"\n",
    "    comparison = []\n",
    "\n",
    "    for k in k_values:\n",
    "        row = {\"k\": k}\n",
    "        for phase_name, base_path in phases.items():\n",
    "            try:\n",
    "                with open(base_path.format(k), \"r\") as f:\n",
    "                    phase_metrics = json.load(f)\n",
    "                row[f\"{phase_name} MAP\"] = phase_metrics[\"map\"]\n",
    "                for m in metrics[1:]: # exclude MAP\n",
    "                    row[f\"{phase_name} avgPre@{m[2:]}\"] = phase_metrics[m]\n",
    "            except FileNotFoundError:\n",
    "                print(f\"⚠️ File not found: {base_path.format(k)}\")\n",
    "        comparison.append(row)\n",
    "\n",
    "    df = pd.DataFrame(comparison)\n",
    "    df.sort_values(\"k\", inplace=True)\n",
    "    df.set_index(\"k\", inplace=True) # Set 'k' column as the index for visualization purposes\n",
    "    display(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = {\n",
    "    \"Phase 1\": \"../results/phase_1/average_metrics_top_{}.json\",\n",
    "    \"Phase 2\": \"../results/phase_2/average_metrics_top_{}.json\",\n",
    "    # \"Phase 3\": \"../results/phase_3/average_metrics_top_{}.json\",\n",
    "    # \"Phase 4\": \"../results/phase_4/average_metrics_top_{}.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phase 1 MAP</th>\n",
       "      <th>Phase 1 avgPre@5</th>\n",
       "      <th>Phase 1 avgPre@10</th>\n",
       "      <th>Phase 1 avgPre@15</th>\n",
       "      <th>Phase 1 avgPre@20</th>\n",
       "      <th>Phase 2 MAP</th>\n",
       "      <th>Phase 2 avgPre@5</th>\n",
       "      <th>Phase 2 avgPre@10</th>\n",
       "      <th>Phase 2 avgPre@15</th>\n",
       "      <th>Phase 2 avgPre@20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.019880</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.014134</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.458667</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.458667</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.038790</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.029628</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.458667</td>\n",
       "      <td>0.447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Phase 1 MAP  Phase 1 avgPre@5  Phase 1 avgPre@10  Phase 1 avgPre@15  \\\n",
       "k                                                                         \n",
       "20     0.019880             0.632               0.59              0.556   \n",
       "30     0.026913             0.632               0.59              0.556   \n",
       "50     0.038790             0.632               0.59              0.556   \n",
       "\n",
       "    Phase 1 avgPre@20  Phase 2 MAP  Phase 2 avgPre@5  Phase 2 avgPre@10  \\\n",
       "k                                                                         \n",
       "20              0.545     0.014134              0.48              0.474   \n",
       "30              0.545     0.019450              0.48              0.474   \n",
       "50              0.545     0.029628              0.48              0.474   \n",
       "\n",
       "    Phase 2 avgPre@15  Phase 2 avgPre@20  \n",
       "k                                         \n",
       "20           0.458667              0.447  \n",
       "30           0.458667              0.447  \n",
       "50           0.458667              0.447  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = compare_phases(phases)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
